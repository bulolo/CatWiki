# Copyright 2024 CatWiki Authors
#
# Licensed under the CatWiki Open Source License (Modified Apache 2.0);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://github.com/CatWiki/CatWiki/blob/main/LICENSE
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""LangGraph ReAct Agent
1. ReAct å¾ªç¯: Agent -> Tools -> Agent ... -> End
2. æ”¯æŒå¤šè½®æ£€ç´¢å’Œæ¨ç†
3. åŠ¨æ€å¼•ç”¨æå–
"""

import logging
import json
from typing import Literal, List, Annotated

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition

from app.schemas.graph_state import ChatGraphState
from app.services.vector_service import VectorService

logger = logging.getLogger(__name__)


# =============================================================================
# å·¥å…·å®šä¹‰
# =============================================================================


@tool
async def search_knowledge_base(query: str) -> str:
    """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯ã€‚
    
    å½“ç”¨æˆ·çš„é—®é¢˜éœ€è¦äº‹å®ä¾æ®ã€æ–‡æ¡£æ”¯æŒæˆ–ä½ ä¸çŸ¥é“ç­”æ¡ˆæ—¶ï¼Œ**å¿…é¡»**ä½¿ç”¨æ­¤å·¥å…·ã€‚
    å¯ä»¥å¤šæ¬¡è°ƒç”¨æ­¤å·¥å…·ä»¥æŸ¥æ‰¾ä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚
    
    Args:
        query: æœç´¢æŸ¥è¯¢è¯ã€‚åº”è¯¥æ˜¯é’ˆå¯¹ç‰¹å®šä¿¡æ¯çš„æ¸…æ™°é—®é¢˜ã€‚
    
    Returns:
        JSON æ ¼å¼çš„å­—ç¬¦ä¸²ï¼ŒåŒ…å«æœç´¢ç»“æœåˆ—è¡¨ã€‚
        æ¯ä¸ªç»“æœåŒ…å« 'content' (å†…å®¹æ‘˜å½•) å’Œ 'metadata' (åŒ…å« title, document_id ç­‰)ã€‚
    """
    logger.info(f"ğŸ”§ [Tool] search_knowledge_base called with query: {query}")

    try:
        # æ‰§è¡Œæ£€ç´¢
        retrieved_docs = await VectorService.retrieve(
            query=query,
            k=5,
            threshold=0.3,
        )

        if not retrieved_docs:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚è¯·å°è¯•å°è¯•ä½¿ç”¨æ›´æ³›åŒ–æˆ–åŒä¹‰çš„å…³é”®è¯æœç´¢ã€‚"

        # æ ¼å¼åŒ–ä¸º JSON ä»¥ä¾¿ LLM å’Œå¼•ç”¨æå–é€»è¾‘ä½¿ç”¨
        results = []
        for doc in retrieved_docs:
            results.append({
                "content": doc.content,
                "metadata": {
                    "document_id": doc.document_id,
                    "title": doc.document_title,
                    "score": doc.score,
                    # å°½å¯èƒ½ä¿ç•™æ›´å¤šå…ƒæ•°æ®ä¾›å¼•ç”¨ä½¿ç”¨
                    **doc.metadata
                }
            })
        
        # è¿”å› JSON å­—ç¬¦ä¸²ï¼ŒLLM å¯ä»¥ç†è§£ç»“æ„åŒ–æ•°æ®
        return json.dumps(results, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ [Tool] Knowledge base search failed: {e}", exc_info=True)
        return f"æœç´¢çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}"


# å·¥å…·åˆ—è¡¨
tools = [search_knowledge_base]


# =============================================================================
# è¾…åŠ©å‡½æ•°ï¼šå¼•ç”¨æå–
# =============================================================================

def extract_citations_from_messages(messages: List[BaseMessage]) -> List[dict]:
    """ä»å†å²æ¶ˆæ¯çš„ ToolMessage ä¸­æå–å¼•ç”¨"""
    citations = {}
    
    for msg in messages:
        if isinstance(msg, ToolMessage) and msg.name == "search_knowledge_base":
            try:
                # å°è¯•è§£æ JSON è¾“å‡º
                content = msg.content
                if isinstance(content, str):
                    results = json.loads(content)
                else:
                    results = content
                
                if isinstance(results, list):
                    for doc in results:
                        if isinstance(doc, dict) and "metadata" in doc:
                            meta = doc["metadata"]
                            doc_id = meta.get("document_id")
                            if doc_id:
                                #å»é‡: ä½¿ç”¨ document_id ä½œä¸º key
                                if doc_id not in citations:
                                    citations[doc_id] = {
                                        "id": str(doc_id),
                                        "title": meta.get("title", "Unknown"),
                                        "siteId": meta.get("site_id"),
                                        "documentId": doc_id,
                                        "score": meta.get("score")
                                    }
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ Failed to parse tool output as JSON for citations: {msg.content[:50]}...")
            except Exception as e:
                logger.error(f"âŒ Error extracting citations: {e}")

    return list(citations.values())


# =============================================================================
# Agent å›¾æ„å»º
# =============================================================================


def create_agent_graph(checkpointer=None, model: ChatOpenAI = None):
    """åˆ›å»º ReAct Agent å›¾
    
    Args:
        checkpointer: å¯é€‰çš„ Checkpointer å®ä¾‹
        model: é…ç½®å¥½çš„ LLM å®ä¾‹ (å¿…é¡»æ”¯æŒ bind_tools)
        
    Returns:
        ç¼–è¯‘åçš„ StateGraph
    """
    if model is None:
        raise ValueError("Model must be provided to create_agent_graph")

    # 1. ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
    model_with_tools = model.bind_tools(tools)

    # 2. å®šä¹‰èŠ‚ç‚¹
    async def agent_node(state: ChatGraphState) -> dict:
        """Agent å†³ç­–èŠ‚ç‚¹"""
        logger.debug("ğŸ¤– [Agent] Thinking...")
        messages = state["messages"]
        
        # ç¡®ä¿ SystemPrompt å­˜åœ¨
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ï¼ŒåŒæ—¶ä¹Ÿèƒ½å¤Ÿè®¿é—®å¤–éƒ¨çŸ¥è¯†åº“ã€‚\n"
            "èƒ½å¤Ÿè¿›è¡Œå¤šæ­¥æ¨ç†å’Œæ£€ç´¢ã€‚\n"
            "è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n"
            "1. å¦‚æœç”¨æˆ·çš„é—®é¢˜éœ€è¦äº‹å®ä¿¡æ¯ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ search_knowledge_base å·¥å…·ã€‚\n"
            "2. å¦‚æœç¬¬ä¸€æ¬¡æœç´¢ç»“æœä¸å®Œæ•´ï¼Œè¯·å°è¯•ä»ä¸åŒè§’åº¦å†æ¬¡æœç´¢ã€‚\n"
            "3. å›ç­”æ—¶è¯·ä¾æ®æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œä¿æŒå®¢è§‚å‡†ç¡®ã€‚\n"
            "4. å¦‚æœæ£€ç´¢ç»“æœä¸ºç©ºï¼Œè¯·è¯šå®å‘ŠçŸ¥ç”¨æˆ·ã€‚\n"
        )
        
        # å¦‚æœå†å²æ¶ˆæ¯ä¸­ç¬¬ä¸€æ¡ä¸æ˜¯ SystemMessageï¼Œåˆ™æ’å…¥
        if not messages or not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content=system_prompt)] + list(messages)
        elif isinstance(messages[0], SystemMessage):
             # ç¡®ä¿ System Prompt å†…å®¹æ˜¯æœ€æ–°çš„ï¼ˆæˆ–è€…æ˜¯åˆå¹¶çš„ï¼‰
             # è¿™é‡Œç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾å¤–éƒ¨è°ƒç”¨è€…å¯èƒ½ä¼šä¼ å…¥ SystemMessageï¼Œæˆ–è€…æˆ‘ä»¬åœ¨è¿™é‡Œå¼ºåˆ¶è¦†ç›–/è¿½åŠ 
             pass

        # è°ƒç”¨æ¨¡å‹
        response = await model_with_tools.ainvoke(messages)
        return {"messages": [response]}

    async def citation_node(state: ChatGraphState) -> dict:
        """åå¤„ç†èŠ‚ç‚¹ï¼šæå–å¼•ç”¨"""
        citations = extract_citations_from_messages(state["messages"])
        logger.info(f"ğŸ“š [Graph] Extracted {len(citations)} citations")
        return {"citations": citations}

    # 3. æ„å»ºå›¾
    graph_builder = StateGraph(ChatGraphState)

    graph_builder.add_node("agent", agent_node)
    graph_builder.add_node("tools", ToolNode(tools))
    
    # å¼•ç”¨æå–èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼Œå¯ä»¥ä½œä¸ºæœ€åä¸€æ­¥ä¼˜åŒ–çŠ¶æ€ï¼‰
    # ä¸ºäº†ç®€åŒ–æµå¼å¤„ç†ï¼Œæˆ‘ä»¬é€šå¸¸ä¸åœ¨å›¾ä¸­æ˜¾å¼åŠ è¿™ä¸ªèŠ‚ç‚¹ä½œä¸ºå¿…é¡»æ­¥éª¤ï¼Œ
    # è€Œæ˜¯è®©å‰ç«¯æˆ–å¤–å±‚ä» messages ä¸­æå–ã€‚ä½†ä¸ºäº† State å®Œæ•´æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åŠ ä¸€ä¸ªç»“æŸå‰çš„èŠ‚ç‚¹ã€‚
    # graph_builder.add_node("process_citations", citation_node)

    # 4. å®šä¹‰è¾¹
    graph_builder.add_edge(START, "agent")
    
    # æ¡ä»¶è¾¹: Agent -> (Tools | END)
    graph_builder.add_conditional_edges(
        "agent",
        tools_condition,
    )
    
    # å¾ªç¯è¾¹: Tools -> Agent
    graph_builder.add_edge("tools", "agent")

    return graph_builder.compile(checkpointer=checkpointer)


# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

def langchain_to_openai(messages: list[BaseMessage], filter_system: bool = False) -> list[dict]:
    """å°† LangChain æ ¼å¼æ¶ˆæ¯è½¬æ¢ä¸º OpenAI æ ¼å¼ (ç”¨äºå‰ç«¯å±•ç¤º)"""
    result = []
    for msg in messages:
        if isinstance(msg, SystemMessage):
            if filter_system:
                continue
            result.append({"role": "system", "content": msg.content})
        elif isinstance(msg, AIMessage):
            # å¤„ç†å·¥å…·è°ƒç”¨
            if msg.tool_calls:
                # OpenAI æ ¼å¼é€šå¸¸ä¸ç›´æ¥å±•ç¤º tool_calls ç»™ç”¨æˆ·çœ‹æ–‡æœ¬ï¼Œ
                # ä½†å¦‚æœæ˜¯æœ€ç»ˆçš„æ¶ˆæ¯å†å²è¿”å›ï¼Œæˆ‘ä»¬å¯èƒ½åªå…³å¿ƒæ–‡æœ¬å†…å®¹ã€‚
                # å¦‚æœæ²¡æœ‰æ–‡æœ¬å†…å®¹ä½†æœ‰ tool_callsï¼Œè¿™é€šå¸¸æ˜¯ä¸­é—´çŠ¶æ€ã€‚
                if msg.content:
                    result.append({"role": "assistant", "content": msg.content})
            else:
                result.append({"role": "assistant", "content": msg.content})
        elif isinstance(msg, HumanMessage):
            result.append({"role": "user", "content": msg.content})
        # ToolMessage é€šå¸¸ä¸ç›´æ¥å±•ç¤ºç»™ç”¨æˆ·ï¼Œæˆ–è€…ä½œä¸º 'tool' role å±•ç¤º
        # å‰ç«¯ UI å¯èƒ½éœ€è¦é€‚é…æ˜¾ç¤º "Thinking..." æˆ– "Used tool: X"
    
    return result
