"""LangGraph å·¥å…·è°ƒç”¨ Agent (æ–¹æ¡ˆ C)

ä½¿ç”¨ LangGraph create_agent æ„å»ºçš„ RAG Agentï¼š
- å°†çŸ¥è¯†åº“æ£€ç´¢å°è£…ä¸ºå·¥å…·
- LLM è‡ªä¸»å†³å®šä½•æ—¶è°ƒç”¨å·¥å…·
- æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨ç›´åˆ°å®Œæˆ
"""

import logging
from typing import Literal

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition

from app.schemas.graph_state import ChatGraphState
from app.services.vector_service import VectorService

logger = logging.getLogger(__name__)


# =============================================================================
# å·¥å…·å®šä¹‰
# =============================================================================

@tool
async def search_knowledge_base(query: str) -> str:
    """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯ã€‚
    
    å½“ç”¨æˆ·æé—®éœ€è¦æŸ¥æ‰¾æ–‡æ¡£ã€èµ„æ–™æˆ–çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚
    
    Args:
        query: æœç´¢æŸ¥è¯¢è¯ï¼Œåº”è¯¥æ˜¯æ¸…æ™°çš„é—®é¢˜æˆ–å…³é”®è¯
        
    Returns:
        æœç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹ï¼ŒåŒ…å«æ ‡é¢˜å’Œå†…å®¹æ‘˜è¦
    """
    logger.info(f"ğŸ”§ [Tool] search_knowledge_base called with query: {query}")
    
    try:
        # æ‰§è¡Œæ£€ç´¢
        retrieved_docs = await VectorService.retrieve(
            query=query,
            k=5,
            threshold=0.3,
        )
        
        if not retrieved_docs:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯æˆ–ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
        
        # æŒ‰ document_id èšåˆ
        doc_map = {}
        for doc in retrieved_docs:
            doc_id = doc.document_id
            if not doc_id:
                continue
            
            if doc_id not in doc_map:
                doc_map[doc_id] = {
                    "title": doc.document_title,
                    "content": [],
                    "score": doc.score,
                }
            doc_map[doc_id]["content"].append(doc.content)
        
        # è½¬æ¢ä¸ºå”¯ä¸€æ–‡æ¡£åˆ—è¡¨
        relevant_docs = []
        seen_ids = set()
        for doc in retrieved_docs:
            doc_id = doc.document_id
            if not doc_id or doc_id in seen_ids:
                continue
            
            seen_ids.add(doc_id)
            info = doc_map[doc_id]
            full_content = "\n...\n".join(info["content"])
            relevant_docs.append({
                "title": info["title"],
                "content": full_content,
                "score": info["score"],
            })
        
        # æ„å»ºè¿”å›ç»“æœ
        result_parts = []
        for i, doc in enumerate(relevant_docs):
            result_parts.append(
                f"[æ–‡æ¡£ {i+1}] {doc['title']}\n"
                f"{doc['content']}\n"
            )
        
        result = "\n---\n".join(result_parts)
        logger.info(f"ğŸ“š [Tool] Found {len(relevant_docs)} relevant documents")
        return result
        
    except Exception as e:
        logger.error(f"âŒ [Tool] Knowledge base search failed: {e}", exc_info=True)
        return f"æœç´¢çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}"


# å·¥å…·åˆ—è¡¨
tools = [search_knowledge_base]


# =============================================================================
# Agent èŠ‚ç‚¹
# =============================================================================

async def agent_node(state: ChatGraphState) -> dict:
    """Agent å…¥å£èŠ‚ç‚¹
    
    æ­¤èŠ‚ç‚¹ä½œä¸ºå›¾çš„å…¥å£ï¼Œä»…è®°å½•æ—¥å¿—å’Œé€ä¼ æ¶ˆæ¯ã€‚
    å®é™…çš„æ¶ˆæ¯å¤„ç†åœ¨ respond_node ä¸­è¿›è¡Œã€‚
    """
    logger.info("ğŸ¤– [Agent] Entering agent_node")
    
    # å…¥å£èŠ‚ç‚¹åªé€ä¼ ï¼Œä¸åšå¤„ç†
    return {}


async def retrieve_for_agent(state: ChatGraphState) -> dict:
    """ä¸º Agent æ‰§è¡Œæ£€ç´¢
    
    ä»æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸­æå–æŸ¥è¯¢å¹¶æ‰§è¡Œæ£€ç´¢ã€‚
    è¿”å›ä¸Šä¸‹æ–‡å’Œå¼•ç”¨æ¥æºã€‚
    """
    logger.info("ğŸ” [Agent] Executing retrieval for agent")
    
    # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    query = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            query = msg.content
            break
    
    if not query:
        return {"context": "", "citations": []}
    
    try:
        # ç›´æ¥è°ƒç”¨ VectorServiceï¼ˆä¸é€šè¿‡å·¥å…·ï¼‰ä»¥è·å–å®Œæ•´çš„æ–‡æ¡£ä¿¡æ¯
        retrieved_docs = await VectorService.retrieve(
            query=query,
            k=5,
            threshold=0.3,
        )
        
        if not retrieved_docs:
            logger.info("ğŸ¤· [Agent] No relevant context found")
            return {"context": "", "citations": []}
        
        # æŒ‰ document_id èšåˆ
        doc_map = {}
        for doc in retrieved_docs:
            doc_id = doc.document_id
            if not doc_id:
                continue
            
            if doc_id not in doc_map:
                doc_map[doc_id] = {
                    "title": doc.document_title,
                    "content": [],
                    "metadata": doc.metadata,
                    "score": doc.score,
                    "document_id": doc_id
                }
            doc_map[doc_id]["content"].append(doc.content)
        
        # è½¬æ¢ä¸ºå”¯ä¸€æ–‡æ¡£åˆ—è¡¨ï¼ˆä¿æŒç›¸å…³æ€§æ’åºï¼‰
        relevant_docs = []
        seen_ids = set()
        for doc in retrieved_docs:
            doc_id = doc.document_id
            if not doc_id or doc_id in seen_ids:
                continue
            
            seen_ids.add(doc_id)
            info = doc_map[doc_id]
            full_content = "\n...\n".join(info["content"])
            
            relevant_docs.append({
                "title": info["title"],
                "content": full_content,
                "metadata": info["metadata"],
                "score": info["score"],
                "document_id": doc_id
            })
        
        logger.info(f"ğŸ“š [Agent] Found {len(retrieved_docs)} chunks -> collapsed to {len(relevant_docs)} unique docs")
        
        # æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        context_parts = []
        for i, doc in enumerate(relevant_docs):
            context_parts.append(
                f"Document [{i+1}] (Title: {doc['title']})\n"
                f"{doc['content']}\n"
            )
        context_str = "\n".join(context_parts)
        
        # æ„å»º Citations
        citations = []
        for i, doc in enumerate(relevant_docs):
            citations.append({
                "id": str(doc["document_id"]),
                "title": doc["title"],
                "siteId": doc["metadata"].get("site_id"),
                "documentId": doc["document_id"],
                "score": doc["score"]
            })
        
        logger.info(f"ğŸ“ [Agent] Prepared {len(citations)} citations")
        
        return {
            "context": context_str,
            "citations": citations
        }
        
    except Exception as e:
        logger.error(f"âŒ [Agent] Retrieval failed: {e}", exc_info=True)
        return {"context": "", "citations": []}


def should_use_tools(state: ChatGraphState) -> Literal["retrieve", "respond"]:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ï¼ˆç®€åŒ–ç‰ˆè·¯ç”±ï¼‰
    
    åœ¨å®Œæ•´çš„ Agent å®ç°ä¸­ï¼Œè¿™ä¸ªåˆ¤æ–­ä¼šç”± LLM é€šè¿‡ tool_calls å†³å®šã€‚
    è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„è§„åˆ™åˆ¤æ–­ã€‚
    """
    # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    query = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            query = msg.content
            break
    
    if not query:
        return "respond"
    
    # ç®€å•çš„è§„åˆ™åˆ¤æ–­
    greetings = ["ä½ å¥½", "hi", "hello", "hey", "å—¨", "æ—©ä¸Šå¥½", "æ™šä¸Šå¥½", "åœ¨å—"]
    query_lower = query.lower().strip()
    
    if len(query) < 20 and any(g in query_lower for g in greetings):
        logger.info("ğŸ‘‹ [Agent] Detected greeting, skipping tools")
        return "respond"
    
    logger.info("ğŸ”§ [Agent] Will use knowledge base tool")
    return "retrieve"


async def respond_node(state: ChatGraphState) -> dict:
    """å“åº”èŠ‚ç‚¹ï¼šå‡†å¤‡æœ€ç»ˆå“åº”çš„æ¶ˆæ¯
    
    æ­¤èŠ‚ç‚¹åœ¨å·¥å…·è°ƒç”¨å®Œæˆåï¼ˆæˆ–è·³è¿‡å·¥å…·æ—¶ï¼‰æ‰§è¡Œï¼Œ
    æ•´ç†æ¶ˆæ¯åˆ—è¡¨ä¾›åç»­ LLM è°ƒç”¨ä½¿ç”¨ã€‚
    """
    logger.info("ğŸ’¬ [Agent] Entering respond_node")
    
    messages = list(state["messages"])
    context = state.get("context", "")
    
    if context:
        # æ³¨å…¥æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“çš„æ™ºèƒ½ AI åŠ©æ‰‹ã€‚\n"
            "è¯·ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µæ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
            "å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·ä½ æ ¹æ®çŸ¥è¯†åº“æ— æ³•å›ç­”ï¼Œä½†ä½ å¯ä»¥å°è¯•æä¾›å¸®åŠ©ã€‚\n"
            "å¦‚æœç›¸å…³ï¼Œè¯·å§‹ç»ˆå¼•ç”¨æ–‡æ¡£æ ‡é¢˜ã€‚\n\n"
            "ä¸Šä¸‹æ–‡ä¿¡æ¯:\n"
            f"{context}\n"
        )
        
        if messages and isinstance(messages[0], SystemMessage):
            messages[0] = SystemMessage(content=messages[0].content + f"\n\n{system_prompt}")
        else:
            messages.insert(0, SystemMessage(content=system_prompt))
        
        logger.debug(f"ğŸ“ [Agent] Context injected into system prompt ({len(context)} chars)")
    else:
        # æ— ä¸Šä¸‹æ–‡æ—¶ä½¿ç”¨é»˜è®¤ System Prompt
        default_prompt = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·å›ç­”å„ç§é—®é¢˜ã€‚"
        if not messages or not isinstance(messages[0], SystemMessage):
            messages.insert(0, SystemMessage(content=default_prompt))
            logger.debug("ğŸ“ [Agent] Default system prompt added (no context)")
    
    return {"messages": messages}


# =============================================================================
# æ„å»º Agent å›¾
# =============================================================================

def create_agent_graph(checkpointer=None):
    """åˆ›å»ºå·¥å…·è°ƒç”¨ Agent å›¾
    
    æµç¨‹:
        START -> agent
        agent --[éœ€è¦å·¥å…·]--> retrieve -> respond -> END
        agent --[ä¸éœ€è¦å·¥å…·]--> respond -> END
    
    Args:
        checkpointer: å¯é€‰çš„ Checkpointer å®ä¾‹ï¼Œç”¨äºæŒä¹…åŒ–çŠ¶æ€
    
    Returns:
        ç¼–è¯‘åçš„ StateGraph
    """
    graph_builder = StateGraph(ChatGraphState)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph_builder.add_node("agent", agent_node)
    graph_builder.add_node("retrieve", retrieve_for_agent)
    graph_builder.add_node("respond", respond_node)
    
    # æ·»åŠ è¾¹
    graph_builder.add_edge(START, "agent")
    
    # æ¡ä»¶è¾¹ï¼šå†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
    graph_builder.add_conditional_edges(
        "agent",
        should_use_tools,
        {
            "retrieve": "retrieve",
            "respond": "respond"
        }
    )
    
    graph_builder.add_edge("retrieve", "respond")
    graph_builder.add_edge("respond", END)
    
    return graph_builder.compile(checkpointer=checkpointer)


# å…¨å±€å•ä¾‹å›¾å®ä¾‹
rag_graph = create_agent_graph()


# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

def messages_to_langchain(messages: list[dict]) -> list[BaseMessage]:
    """å°† OpenAI æ ¼å¼æ¶ˆæ¯è½¬æ¢ä¸º LangChain æ ¼å¼"""
    result = []
    for msg in messages:
        role = msg.get("role", "user")
        content = msg.get("content", "")
        
        if role == "system":
            result.append(SystemMessage(content=content))
        elif role == "assistant":
            result.append(AIMessage(content=content))
        else:
            result.append(HumanMessage(content=content))
    
    return result


def langchain_to_openai(messages: list[BaseMessage]) -> list[dict]:
    """å°† LangChain æ ¼å¼æ¶ˆæ¯è½¬æ¢ä¸º OpenAI æ ¼å¼"""
    result = []
    for msg in messages:
        if isinstance(msg, SystemMessage):
            result.append({"role": "system", "content": msg.content})
        elif isinstance(msg, AIMessage):
            result.append({"role": "assistant", "content": msg.content})
        elif isinstance(msg, HumanMessage):
            result.append({"role": "user", "content": msg.content})
    
    return result


def get_tools():
    """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆä¾›å¤–éƒ¨ä½¿ç”¨ï¼‰"""
    return tools
