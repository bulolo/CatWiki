# Copyright 2024 CatWiki Authors
#
# Licensed under the CatWiki Open Source License (Modified Apache 2.0);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://github.com/CatWiki/CatWiki/blob/main/LICENSE
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""LangGraph ReAct Agent
1. ReAct å¾ªç¯: Agent -> Tools -> Agent ... -> End
2. æ”¯æŒå¤šè½®æ£€ç´¢å’Œæ¨ç†
3. åŠ¨æ€å¼•ç”¨æå–
"""

import logging
import json
from typing import Literal, List, Annotated

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode

from app.schemas.graph_state import ChatGraphState
from app.services.vector_service import VectorService

logger = logging.getLogger(__name__)

# æœ€å¤§è¿­ä»£æ¬¡æ•°é™åˆ¶ï¼Œé˜²æ­¢ Agent æ— é™å¾ªç¯ï¼ˆä»é…ç½®è¯»å–ï¼‰
from app.core.config import settings
MAX_ITERATIONS = settings.AGENT_MAX_ITERATIONS


# =============================================================================
# å·¥å…·å®šä¹‰
# =============================================================================


@tool
async def search_knowledge_base(query: str) -> str:
    """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯ã€‚
    
    å½“ç”¨æˆ·çš„é—®é¢˜éœ€è¦äº‹å®ä¾æ®ã€æ–‡æ¡£æ”¯æŒæˆ–ä½ ä¸çŸ¥é“ç­”æ¡ˆæ—¶ï¼Œ**å¿…é¡»**ä½¿ç”¨æ­¤å·¥å…·ã€‚
    å¯ä»¥å¤šæ¬¡è°ƒç”¨æ­¤å·¥å…·ä»¥æŸ¥æ‰¾ä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚
    
    Args:
        query: æœç´¢æŸ¥è¯¢è¯ã€‚åº”è¯¥æ˜¯é’ˆå¯¹ç‰¹å®šä¿¡æ¯çš„æ¸…æ™°é—®é¢˜ã€‚
    
    Returns:
        JSON æ ¼å¼çš„å­—ç¬¦ä¸²ï¼ŒåŒ…å«æœç´¢ç»“æœåˆ—è¡¨ã€‚
        æ¯ä¸ªç»“æœåŒ…å« 'content' (å†…å®¹æ‘˜å½•) å’Œ 'metadata' (åŒ…å« title, document_id ç­‰)ã€‚
    """
    logger.info(f"ğŸ”§ [Tool] search_knowledge_base called with query: {query}")

    try:
        # æ‰§è¡Œæ£€ç´¢
        retrieved_docs = await VectorService.retrieve(
            query=query,
            k=5,
            threshold=0.3,
        )

        if not retrieved_docs:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚è¯·å°è¯•å°è¯•ä½¿ç”¨æ›´æ³›åŒ–æˆ–åŒä¹‰çš„å…³é”®è¯æœç´¢ã€‚"

        # æ ¼å¼åŒ–ä¸º JSON ä»¥ä¾¿ LLM å’Œå¼•ç”¨æå–é€»è¾‘ä½¿ç”¨
        results = []
        for doc in retrieved_docs:
            results.append({
                "content": doc.content,
                "metadata": {
                    "document_id": doc.document_id,
                    "title": doc.document_title,
                    "score": doc.score,
                    # å°½å¯èƒ½ä¿ç•™æ›´å¤šå…ƒæ•°æ®ä¾›å¼•ç”¨ä½¿ç”¨
                    **doc.metadata
                }
            })
        
        # è¿”å› JSON å­—ç¬¦ä¸²ï¼ŒLLM å¯ä»¥ç†è§£ç»“æ„åŒ–æ•°æ®
        return json.dumps(results, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ [Tool] Knowledge base search failed: {e}", exc_info=True)
        return f"æœç´¢çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}"


# å·¥å…·åˆ—è¡¨
tools = [search_knowledge_base]


# =============================================================================
# è¾…åŠ©å‡½æ•°ï¼šå¼•ç”¨æå–
# =============================================================================

def extract_citations_from_messages(messages: List[BaseMessage]) -> List[dict]:
    """ä»å†å²æ¶ˆæ¯çš„ ToolMessage ä¸­æå–å¼•ç”¨"""
    citations = {}
    
    for msg in messages:
        if isinstance(msg, ToolMessage) and msg.name == "search_knowledge_base":
            try:
                # å°è¯•è§£æ JSON è¾“å‡º
                content = msg.content
                if isinstance(content, str):
                    results = json.loads(content)
                else:
                    results = content
                
                if isinstance(results, list):
                    for doc in results:
                        if isinstance(doc, dict) and "metadata" in doc:
                            meta = doc["metadata"]
                            doc_id = meta.get("document_id")
                            if doc_id:
                                #å»é‡: ä½¿ç”¨ document_id ä½œä¸º key
                                if doc_id not in citations:
                                    citations[doc_id] = {
                                        "id": str(doc_id),
                                        "title": meta.get("title", "Unknown"),
                                        "siteId": meta.get("site_id"),
                                        "documentId": doc_id,
                                        "score": meta.get("score")
                                    }
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ Failed to parse tool output as JSON for citations: {msg.content[:50]}...")
            except Exception as e:
                logger.error(f"âŒ Error extracting citations: {e}")

    return list(citations.values())


# =============================================================================
# Agent å›¾æ„å»º
# =============================================================================


def create_agent_graph(checkpointer=None, model: ChatOpenAI = None):
    """åˆ›å»º ReAct Agent å›¾
    
    Args:
        checkpointer: å¯é€‰çš„ Checkpointer å®ä¾‹
        model: é…ç½®å¥½çš„ LLM å®ä¾‹ (å¿…é¡»æ”¯æŒ bind_tools)
        
    Returns:
        ç¼–è¯‘åçš„ StateGraph
    """
    if model is None:
        raise ValueError("Model must be provided to create_agent_graph")

    # 1. ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
    model_with_tools = model.bind_tools(tools)

    # 2. å®šä¹‰èŠ‚ç‚¹
    async def agent_node(state: ChatGraphState) -> dict:
        """Agent å†³ç­–èŠ‚ç‚¹"""
        logger.debug("ğŸ¤– [Agent] Thinking...")
        messages = state["messages"]
        
        # ç¡®ä¿ SystemPrompt å­˜åœ¨
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ï¼ŒåŒæ—¶ä¹Ÿèƒ½å¤Ÿè®¿é—®å¤–éƒ¨çŸ¥è¯†åº“ã€‚\n"
            "èƒ½å¤Ÿè¿›è¡Œå¤šæ­¥æ¨ç†å’Œæ£€ç´¢ã€‚\n"
            "è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n"
            "1. å¦‚æœç”¨æˆ·çš„é—®é¢˜éœ€è¦äº‹å®ä¿¡æ¯ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ search_knowledge_base å·¥å…·ã€‚\n"
            "2. å¦‚æœç¬¬ä¸€æ¬¡æœç´¢ç»“æœä¸å®Œæ•´ï¼Œè¯·å°è¯•ä»ä¸åŒè§’åº¦å†æ¬¡æœç´¢ã€‚\n"
            "3. å›ç­”æ—¶è¯·ä¾æ®æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œä¿æŒå®¢è§‚å‡†ç¡®ã€‚\n"
            "4. å¦‚æœæ£€ç´¢ç»“æœä¸ºç©ºï¼Œè¯·è¯šå®å‘ŠçŸ¥ç”¨æˆ·ã€‚\n"
        )
        
        # å¦‚æœå†å²æ¶ˆæ¯ä¸­ç¬¬ä¸€æ¡ä¸æ˜¯ SystemMessageï¼Œåˆ™æ’å…¥
        if not messages or not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content=system_prompt)] + list(messages)
        elif isinstance(messages[0], SystemMessage):
             # ç¡®ä¿ System Prompt å†…å®¹æ˜¯æœ€æ–°çš„ï¼ˆæˆ–è€…æ˜¯åˆå¹¶çš„ï¼‰
             # è¿™é‡Œç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾å¤–éƒ¨è°ƒç”¨è€…å¯èƒ½ä¼šä¼ å…¥ SystemMessageï¼Œæˆ–è€…æˆ‘ä»¬åœ¨è¿™é‡Œå¼ºåˆ¶è¦†ç›–/è¿½åŠ 
             pass

        # è°ƒç”¨æ¨¡å‹
        response = await model_with_tools.ainvoke(messages)
        return {"messages": [response]}

    async def citation_node(state: ChatGraphState) -> dict:
        """åå¤„ç†èŠ‚ç‚¹ï¼šæå–å¼•ç”¨"""
        citations = extract_citations_from_messages(state["messages"])
        logger.info(f"ğŸ“š [Graph] Extracted {len(citations)} citations")
        return {"citations": citations}

    # 3. æ„å»ºå›¾
    graph_builder = StateGraph(ChatGraphState)

    # å·¥å…·èŠ‚ç‚¹åŒ…è£…å™¨ï¼šé€’å¢è¿­ä»£è®¡æ•° + æ£€æµ‹ç©ºç»“æœ
    tool_node = ToolNode(tools)
    
    # è¿ç»­ç©ºç»“æœç»ˆæ­¢é˜ˆå€¼ï¼ˆä»é…ç½®è¯»å–ï¼‰
    MAX_CONSECUTIVE_EMPTY = settings.AGENT_MAX_CONSECUTIVE_EMPTY
    
    async def tools_wrapper_node(state: ChatGraphState) -> dict:
        """å·¥å…·èŠ‚ç‚¹åŒ…è£…å™¨ï¼Œæ‰§è¡Œå·¥å…·å¹¶è¿½è¸ªè¿­ä»£è®¡æ•°å’Œç©ºç»“æœ"""
        # è°ƒç”¨åŸå§‹å·¥å…·èŠ‚ç‚¹
        result = await tool_node.ainvoke(state)
        
        # é€’å¢è¿­ä»£è®¡æ•°
        current_count = state.get("iteration_count", 0)
        result["iteration_count"] = current_count + 1
        
        # æ£€æµ‹å·¥å…·è¿”å›æ˜¯å¦ä¸ºç©ºç»“æœ
        consecutive_empty = state.get("consecutive_empty_count", 0)
        is_empty_result = False
        
        # æ£€æŸ¥æœ€åä¸€æ¡å·¥å…·æ¶ˆæ¯æ˜¯å¦ä¸ºç©ºç»“æœ
        if result.get("messages"):
            last_tool_msg = result["messages"][-1] if result["messages"] else None
            if last_tool_msg:
                content = getattr(last_tool_msg, "content", "")
                # æ£€æµ‹ç©ºç»“æœæ ‡å¿—
                if "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£" in content or content == "[]":
                    is_empty_result = True
        
        if is_empty_result:
            result["consecutive_empty_count"] = consecutive_empty + 1
            logger.debug(f"ğŸ”„ [Graph] Empty result, consecutive count: {result['consecutive_empty_count']}/{MAX_CONSECUTIVE_EMPTY}")
        else:
            result["consecutive_empty_count"] = 0  # é‡ç½®
        
        logger.debug(f"ğŸ”„ [Graph] Iteration count: {result['iteration_count']}/{MAX_ITERATIONS}")
        return result

    # æ¡ä»¶è·¯ç”±å‡½æ•°ï¼šæ£€æŸ¥è¿­ä»£æ¬¡æ•°é™åˆ¶ + è¿ç»­ç©ºç»“æœ
    def route_after_agent(state: ChatGraphState) -> Literal["tools", "__end__"]:
        """Agent åçš„è·¯ç”±å†³ç­–ï¼ŒåŒ…å«è¿­ä»£æ¬¡æ•°å’Œè¿ç»­ç©ºç»“æœæ£€æŸ¥"""
        messages = state["messages"]
        last_message = messages[-1] if messages else None
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        if last_message and hasattr(last_message, "tool_calls") and last_message.tool_calls:
            # æ£€æŸ¥è¿­ä»£æ¬¡æ•°
            current_count = state.get("iteration_count", 0)
            if current_count >= MAX_ITERATIONS:
                logger.warning(
                    f"âš ï¸ [Graph] Max iterations ({MAX_ITERATIONS}) reached, forcing end"
                )
                return "__end__"
            
            # æ£€æŸ¥è¿ç»­ç©ºç»“æœ
            consecutive_empty = state.get("consecutive_empty_count", 0)
            if consecutive_empty >= MAX_CONSECUTIVE_EMPTY:
                logger.warning(
                    f"âš ï¸ [Graph] {MAX_CONSECUTIVE_EMPTY} consecutive empty results, stopping early"
                )
                return "__end__"
            
            return "tools"
        
        return "__end__"

    graph_builder.add_node("agent", agent_node)
    graph_builder.add_node("tools", tools_wrapper_node)
    
    # å¼•ç”¨æå–èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼Œå¯ä»¥ä½œä¸ºæœ€åä¸€æ­¥ä¼˜åŒ–çŠ¶æ€ï¼‰
    # ä¸ºäº†ç®€åŒ–æµå¼å¤„ç†ï¼Œæˆ‘ä»¬é€šå¸¸ä¸åœ¨å›¾ä¸­æ˜¾å¼åŠ è¿™ä¸ªèŠ‚ç‚¹ä½œä¸ºå¿…é¡»æ­¥éª¤ï¼Œ
    # è€Œæ˜¯è®©å‰ç«¯æˆ–å¤–å±‚ä» messages ä¸­æå–ã€‚ä½†ä¸ºäº† State å®Œæ•´æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åŠ ä¸€ä¸ªç»“æŸå‰çš„èŠ‚ç‚¹ã€‚
    # graph_builder.add_node("process_citations", citation_node)

    # 4. å®šä¹‰è¾¹
    graph_builder.add_edge(START, "agent")
    
    # æ¡ä»¶è¾¹: Agent -> (Tools | END)ï¼ŒåŒ…å«è¿­ä»£æ¬¡æ•°æ£€æŸ¥
    graph_builder.add_conditional_edges(
        "agent",
        route_after_agent,
    )
    
    # å¾ªç¯è¾¹: Tools -> Agent
    graph_builder.add_edge("tools", "agent")

    return graph_builder.compile(checkpointer=checkpointer)


# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

def langchain_to_openai(messages: list[BaseMessage], filter_system: bool = False) -> list[dict]:
    """å°† LangChain æ ¼å¼æ¶ˆæ¯è½¬æ¢ä¸º OpenAI æ ¼å¼ (å®Œå…¨å…¼å®¹ tool calling)
    
    æ”¯æŒè½¬æ¢ï¼š
    - SystemMessage -> {"role": "system", "content": ...}
    - HumanMessage -> {"role": "user", "content": ...}
    - AIMessage -> {"role": "assistant", "content": ..., "tool_calls": [...]}
    - ToolMessage -> {"role": "tool", "tool_call_id": ..., "content": ...}
    """
    result = []
    for msg in messages:
        if isinstance(msg, SystemMessage):
            if filter_system:
                continue
            result.append({"role": "system", "content": msg.content})
        
        elif isinstance(msg, AIMessage):
            message_dict = {"role": "assistant"}
            
            # å¤„ç† contentï¼ˆå¯èƒ½ä¸ºç©ºå­—ç¬¦ä¸²æˆ– Noneï¼‰
            if msg.content:
                message_dict["content"] = msg.content
            else:
                message_dict["content"] = None
            
            # å¤„ç† tool_callsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if msg.tool_calls:
                tool_calls_list = []
                for tc in msg.tool_calls:
                    # LangChain çš„ tool_call ç»“æ„è½¬æ¢ä¸º OpenAI æ ¼å¼
                    tool_call_dict = {
                        "id": tc.get("id", ""),
                        "type": "function",
                        "function": {
                            "name": tc.get("name", ""),
                            "arguments": json.dumps(tc.get("args", {}), ensure_ascii=False)
                        }
                    }
                    tool_calls_list.append(tool_call_dict)
                message_dict["tool_calls"] = tool_calls_list
            
            result.append(message_dict)
        
        elif isinstance(msg, HumanMessage):
            result.append({"role": "user", "content": msg.content})
        
        elif isinstance(msg, ToolMessage):
            # OpenAI æ ¼å¼çš„ tool role æ¶ˆæ¯
            result.append({
                "role": "tool",
                "tool_call_id": msg.tool_call_id,
                "content": msg.content if isinstance(msg.content, str) else json.dumps(msg.content, ensure_ascii=False)
            })
    
    return result
