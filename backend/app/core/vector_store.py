"""å‘é‡å­˜å‚¨ç®¡ç†å™¨

ä½¿ç”¨ langchain-postgres æœ€ä½³å®è·µå®ç°
"""

import logging
from typing import Optional
from urllib.parse import quote_plus

from langchain_core.documents import Document as LangChainDocument
from langchain_postgres import PGEngine, PGVectorStore
from langchain_postgres.v2.engine import Column
from sqlalchemy.ext.asyncio import create_async_engine

from app.core.config import settings

logger = logging.getLogger(__name__)


class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    åŸºäº langchain-postgres æ–‡æ¡£æœ€ä½³å®è·µï¼š
    - ä½¿ç”¨ PGEngine.from_engine() ç®¡ç†è¿æ¥æ± 
    - ä½¿ç”¨ ainit_vectorstore_table() åˆå§‹åŒ–è¡¨
    - ä½¿ç”¨ PGVectorStore.create() åˆ›å»ºå‘é‡å­˜å‚¨
    """

    _instance: Optional['VectorStoreManager'] = None
    _initialized: bool = False

    def __init__(self, collection_name: str = "catwiki_documents"):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
        self.collection_name = collection_name
        self._engine: PGEngine | None = None
        self._sa_engine = None  # Underlying SQLAlchemy AsyncEngine
        self._vector_store: PGVectorStore | None = None

        # åˆå§‹åŒ– OpenAI å…¼å®¹ Embeddings
        from app.core.embeddings import OpenAICompatibleEmbeddings

        self.embeddings = OpenAICompatibleEmbeddings(
            model=settings.AI_EMBEDDING_MODEL,
            api_key=settings.AI_EMBEDDING_API_KEY,
            base_url=settings.AI_EMBEDDING_API_BASE
        )

        logger.info(f"å‘é‡å­˜å‚¨é…ç½®: model={settings.AI_EMBEDDING_MODEL}, dim={settings.AI_EMBEDDING_DIMENSION}")

    async def _ensure_initialized(self):
        """ç¡®ä¿å‘é‡å­˜å‚¨å·²åˆå§‹åŒ–ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._initialized:
            return

        try:
            # æ„å»ºå¼‚æ­¥è¿æ¥å­—ç¬¦ä¸²
            encoded_user = quote_plus(settings.POSTGRES_USER)
            encoded_password = quote_plus(settings.POSTGRES_PASSWORD)
            async_conn_str = (
                f"postgresql+asyncpg://{encoded_user}:{encoded_password}"
                f"@{settings.POSTGRES_SERVER}:{settings.POSTGRES_PORT}/{settings.POSTGRES_DB}"
            )

            # åˆ›å»ºé…ç½®å¥½çš„ AsyncEngine
            logger.debug("åˆ›å»ºæ•°æ®åº“å¼•æ“...")
            async_engine = create_async_engine(
                async_conn_str,
                pool_size=10,
                max_overflow=20,
                pool_timeout=30,
                pool_recycle=1800,
                pool_pre_ping=True,
                pool_reset_on_return="commit",
                echo=False,
                future=True,
            )
            
            self._sa_engine = async_engine

            # ä½¿ç”¨ PGEngine.from_engine()ï¼ˆæ–‡æ¡£æ¨èï¼‰
            logger.debug("åˆ›å»º PGEngine...")
            self._engine = PGEngine.from_engine(engine=async_engine)

            # å®šä¹‰å…ƒæ•°æ®åˆ—
            metadata_columns = [
                Column(name="source", data_type="TEXT", nullable=True),
                Column(name="id", data_type="TEXT", nullable=True),
                Column(name="site_id", data_type="INTEGER", nullable=True),
            ]

            # åˆå§‹åŒ–å‘é‡å­˜å‚¨è¡¨
            logger.debug(f"æ£€æŸ¥å¹¶åˆå§‹åŒ–å‘é‡å­˜å‚¨è¡¨: {self.collection_name}")
            try:
                from sqlalchemy import text
                # å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œé¿å… ainit_vectorstore_table æŠ›å‡º DuplicateTable å¼‚å¸¸è§¦å‘ Postgres æ—¥å¿—æŠ¥é”™
                check_sql = text(f"SELECT 1 FROM information_schema.tables WHERE table_name = :table")
                async with self._sa_engine.connect() as conn:
                    result = await conn.execute(check_sql, {"table": self.collection_name})
                    exists = result.fetchone() is not None
                
                if not exists:
                    logger.info(f"åˆ›å»ºå‘é‡å­˜å‚¨è¡¨: {self.collection_name}")
                    await self._engine.ainit_vectorstore_table(
                        table_name=self.collection_name,
                        vector_size=settings.AI_EMBEDDING_DIMENSION,
                        metadata_columns=metadata_columns,
                    )
                else:
                    logger.debug(f"è¡¨ {self.collection_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–å‘é‡å­˜å‚¨è¡¨å¤±è´¥: {e}")
                # ä¾ç„¶å°è¯•æ•è·å¹¶å‘æƒ…å†µä¸‹çš„ "already exists"
                if "already exists" not in str(e) and "DuplicateTable" not in str(e):
                    raise e

            # åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹
            logger.debug("åˆ›å»º PGVectorStore å®ä¾‹...")
            self.optimized_columns = ["source", "id", "site_id"]
            
            self._vector_store = await PGVectorStore.create(
                engine=self._engine,
                table_name=self.collection_name,
                embedding_service=self.embeddings,
                metadata_columns=self.optimized_columns,
            )

            self._initialized = True
            logger.info(f"âœ… [VectorStore] åˆå§‹åŒ–å®Œæˆ (Model: {settings.AI_EMBEDDING_MODEL})")

        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    @classmethod
    async def get_instance(cls) -> 'VectorStoreManager':
        """è·å–å•ä¾‹å®ä¾‹ï¼ˆå¼‚æ­¥ï¼‰"""
        if cls._instance is None:
            cls._instance = cls()

        # ç¡®ä¿å·²åˆå§‹åŒ–
        await cls._instance._ensure_initialized()
        return cls._instance

    async def reload_credentials(self, config_value: dict) -> None:
        """
        çƒ­æ›´æ–°å‘é‡å­˜å‚¨çš„å‡­è¯ä¿¡æ¯
        :param config_value: æœ€æ–°çš„ system_config["config_value"]
        """
        await self._ensure_initialized()
        
        try:
            # æå– Embedding é…ç½®
            # é€»è¾‘ç±»ä¼¼ dynamic_configï¼Œä½†ä¸“é—¨é’ˆå¯¹ embedding
            
            new_model = ""
            new_api_key = ""
            new_base_url = ""
            
            # 1. å°è¯•è¯»å–æ‰å¹³é…ç½®
            embedding_conf = config_value.get("embedding", {})
            
            # 2. å…¼å®¹æ—§ç»“æ„
            if not embedding_conf and "manualConfig" in config_value:
                 embedding_conf = config_value.get("manualConfig", {}).get("embedding", {})

            if embedding_conf.get("apiKey") and embedding_conf.get("baseUrl"):
                new_api_key = embedding_conf.get("apiKey")
                new_base_url = embedding_conf.get("baseUrl")
                new_model = embedding_conf.get("model", "")
            else:
                 logger.warning("âš ï¸ [VectorStore] Reload triggered but Config for Embedding is missing or incomplete.")

            if new_api_key and new_base_url:
                logger.info(f"ğŸ”„ [VectorStore] Reloading credentials. Model: {new_model}, Base: {new_base_url}")
                
                # æ›´æ–° Embeddings å®ä¾‹
                if hasattr(self.embeddings, "update_credentials"):
                    self.embeddings.update_credentials(
                        api_key=new_api_key,
                        base_url=new_base_url,
                        model=new_model
                    )
                else:
                    logger.warning("âš ï¸ Current embeddings instance does not support update_credentials")
            else:
                logger.warning("âŒ [VectorStore] Failed to reload: Missing API Key or Base URL in config.")

        except Exception as e:
            logger.error(f"âŒ Failed to reload vector store credentials: {e}")

    async def add_documents(
        self,
        documents: list[LangChainDocument],
        ids: list[str],
        batch_size: int = 100
    ) -> list[str]:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨ï¼ˆæ”¯æŒåˆ†æ‰¹å¤„ç†ï¼‰"""
        await self._ensure_initialized()

        try:
            import time
            start_time = time.time()
            total = len(documents)
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, total, batch_size):
                batch_docs = documents[i : i + batch_size]
                batch_ids = ids[i : i + batch_size]
                
                await self._vector_store.aadd_documents(
                    documents=batch_docs,
                    ids=batch_ids
                )
                logger.debug(f"å·²å­˜å‚¨æ‰¹æ¬¡ {i // batch_size + 1}/{(total + batch_size - 1) // batch_size}")

            elapsed = time.time() - start_time
            logger.info(f"âœ… [VectorStore] å·²å­˜å‚¨ {total} ä¸ªæ–‡æ¡£ | è€—æ—¶: {elapsed:.3f}s")
            return ids

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
            raise

    async def delete_documents(self, ids: list[str]) -> None:
        """ä»å‘é‡å­˜å‚¨åˆ é™¤æ–‡æ¡£"""
        await self._ensure_initialized()

        try:
            logger.info(f"å¼€å§‹åˆ é™¤ {len(ids)} ä¸ªæ–‡æ¡£")

            # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•åˆ é™¤æ–‡æ¡£
            await self._vector_store.adelete(ids=ids)

            logger.info("âœ… æˆåŠŸåˆ é™¤æ–‡æ¡£")

        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
            raise

    async def delete_by_metadata(self, key: str, value: str) -> None:
        """æ ¹æ®å…ƒæ•°æ®åˆ é™¤æ–‡æ¡£"""
        await self._ensure_initialized()

        try:
            logger.info(f"å¼€å§‹æ ¹æ®å…ƒæ•°æ®åˆ é™¤æ–‡æ¡£: {key}={value}")

            from sqlalchemy import text

            # è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦å¯ä»¥ä½¿ç”¨ä¼˜åŒ–åˆ—
            # å¦‚æœ key æ˜¯æˆ‘ä»¬åœ¨ init ä¸­å®šä¹‰çš„ optimized_columns ä¹‹ä¸€ï¼Œç›´æ¥ä½¿ç”¨ SQL åˆ—æŸ¥è¯¢
            if key in self.optimized_columns:
                sql = text(f"DELETE FROM {self.collection_name} WHERE {key} = :value")
                logger.debug(f"ä½¿ç”¨ä¼˜åŒ–åˆ—åˆ é™¤: {key}")
            else:
                # å¦åˆ™ä½¿ç”¨ JSONB æŸ¥è¯¢
                sql = text(f"DELETE FROM {self.collection_name} WHERE langchain_metadata->>'{key}' = :value")
                logger.debug(f"ä½¿ç”¨ Metadata JSON åˆ é™¤: {key}")

            async with self._sa_engine.connect() as conn:
                await conn.execute(sql, {"value": value})
                await conn.commit()

            logger.info(f"âœ… æˆåŠŸåˆ é™¤å…ƒæ•°æ® {key}={value} çš„ç›¸å…³å‘é‡")

        except Exception as e:
            logger.error(f"æ ¹æ®å…ƒæ•°æ®åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
            raise

    async def similarity_search(
        self,
        query: str,
        k: int = 5,
        filter: dict | None = None
    ) -> list[LangChainDocument]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        await self._ensure_initialized()

        try:
            logger.info(f"æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢: query='{query[:50]}...', k={k}")

            results = await self._vector_store.asimilarity_search(
                query=query,
                k=k,
                filter=filter
            )

            logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results

        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}", exc_info=True)
            raise

    async def similarity_search_with_score(
        self,
        query: str,
        k: int = 5,
        filter: dict | None = None
    ) -> list[tuple[LangChainDocument, float]]:
        """å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æœç´¢"""
        await self._ensure_initialized()

        try:
            results = await self._vector_store.asimilarity_search_with_score(
                query=query,
                k=k,
                filter=filter
            )
            return results

        except Exception as e:
            logger.error(f"å‘é‡æœç´¢ï¼ˆå¸¦åˆ†æ•°ï¼‰å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_chunks_by_metadata(self, key: str, value: str) -> list[dict]:
        """æ ¹æ®å…ƒæ•°æ®è·å–æ–‡æ¡£ç‰‡æ®µ"""
        await self._ensure_initialized()
        try:
            from sqlalchemy import text
            
            # è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦å¯ä»¥ä½¿ç”¨ä¼˜åŒ–åˆ—
            where_clause = ""
            if key in self.optimized_columns:
                where_clause = f"{key} = :value"
            else:
                where_clause = f"langchain_metadata->>'{key}' = :value"

            sql = text(f"""
                SELECT langchain_id, content, langchain_metadata 
                FROM {self.collection_name} 
                WHERE {where_clause} 
                ORDER BY (langchain_metadata->>'chunk_index')::int ASC
            """)
            
            async with self._sa_engine.connect() as conn:
                result = await conn.execute(sql, {"value": value})
                rows = result.fetchall()
                
            return [
                {
                    "id": str(row.langchain_id),
                    "content": row.content,
                    "metadata": row.langchain_metadata
                }
                for row in rows
            ]
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£ç‰‡æ®µå¤±è´¥: {e}", exc_info=True)
            return []

    async def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self._engine and hasattr(self._engine, '_pool'):
            try:
                await self._engine._pool.dispose()
                logger.info("âœ… å‘é‡å­˜å‚¨è¿æ¥å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")

