# Copyright 2024 CatWiki Authors
#
# Licensed under the CatWiki Open Source License (Modified Apache 2.0);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://github.com/CatWiki/CatWiki/blob/main/LICENSE
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
æ–‡æ¡£ç®¡ç† API ç«¯ç‚¹
"""

import logging
import time

import shutil
import tempfile
from pathlib import Path

from fastapi import APIRouter, BackgroundTasks, Depends, Query, status, UploadFile, File, Form
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.deps import get_current_active_user
from app.core.exceptions import BadRequestException, NotFoundException
from app.core.utils import Paginator, build_collection_map, enrich_document_dict, get_vector_id
from app.crud import crud_collection, crud_document, crud_site
from app.db.database import AsyncSessionLocal, get_db
from app.models.document import Document as DocumentModel
from app.models.document import VectorStatus, DocumentStatus
from app.models.user import User
from app.schemas import ApiResponse, PaginatedResponse
from app.schemas.document import (
    Document,
    DocumentCreate,
    DocumentUpdate,
    VectorizeRequest,
    VectorizeResponse,
    VectorRetrieveRequest,
    VectorRetrieveResponse,
    VectorRetrieveResult,
)
from app.core.doc_processor import DocProcessorFactory
from app.schemas.system_config import DocProcessorConfig
from app.crud.system_config import crud_system_config

router = APIRouter()
logger = logging.getLogger(__name__)


# ============ è¾…åŠ©å‡½æ•° ============


def can_vectorize(document: DocumentModel) -> bool:
    """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å¯ä»¥å‘é‡åŒ–"""
    return document.vector_status in (
        VectorStatus.NONE,
        VectorStatus.FAILED,
        VectorStatus.COMPLETED,
        None,
    )


# ============ å‘é‡åŒ–åå°ä»»åŠ¡ ============


async def process_vectorization_task(document_id: int):
    """
    å¤„ç†æ–‡æ¡£å‘é‡åŒ–ä»»åŠ¡ï¼ˆå¼‚æ­¥åå°ä»»åŠ¡ï¼‰
    æµç¨‹ï¼špending -> processing -> [add to vector store] -> completed
    """
    import time

    task_start_time = time.time()
    logger.info(f"ğŸ”„ [Task] å¼€å§‹å¤„ç†å‘é‡åŒ–ä»»åŠ¡ | DocID: {document_id}")

    # åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯
    async with AsyncSessionLocal() as db:
        try:
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨ä¸”çŠ¶æ€ä¸º pending
            document = await crud_document.get(db, id=document_id)
            if not document:
                logger.warning(f"âš ï¸ æ–‡æ¡£ {document_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡å‘é‡åŒ–")
                return

            if document.vector_status != VectorStatus.PENDING:
                logger.warning(
                    f"âš ï¸ æ–‡æ¡£ {document_id} çŠ¶æ€ä¸ä¸º pending ({document.vector_status})ï¼Œè·³è¿‡å‘é‡åŒ–"
                )
                return

            # æ›´æ–°ä¸º processing çŠ¶æ€
            await crud_document.update_vector_status(
                db, document_id=document_id, status=VectorStatus.PROCESSING
            )

            # è·å–æ–‡æ¡£å†…å®¹
            if not document.content:
                logger.warning(f"âš ï¸ æ–‡æ¡£ {document_id} å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å‘é‡åŒ–")
                await crud_document.update_vector_status(
                    db, document_id=document_id, status=VectorStatus.FAILED, error="æ–‡æ¡£å†…å®¹ä¸ºç©º"
                )
                return

            # å‡†å¤‡ LangChain æ–‡æ¡£å¯¹è±¡

            from app.core.vector_store import VectorStoreManager

            # åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨
            vector_store = await VectorStoreManager.get_instance()

            # 1. å‡†å¤‡å…ƒæ•°æ®
            base_metadata = {
                "source": "document",
                "id": str(document.id),
                "title": document.title,
                "author": document.author,
                "site_id": document.site_id,
                "collection_id": document.collection_id,
            }

            # 2. æ–‡æœ¬åˆ‡ç‰‡
            from langchain_text_splitters import RecursiveCharacterTextSplitter

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
            )

            # ä½¿ç”¨ create_documents è‡ªåŠ¨å¤„ç†åˆ‡ç‰‡
            chunks = text_splitter.create_documents(
                texts=[document.content], metadatas=[base_metadata]
            )

            logger.info(f"ğŸ“„ æ–‡æ¡£ {document_id} å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")

            # 3. ç”Ÿæˆç¡®å®šæ€§ ID (uuid5)
            # å¼•å…¥ uuid å’Œ NAMESPACE
            import uuid

            from app.core.vector_store import VectorStoreManager
            from app.core.utils import NAMESPACE_CATWIKI

            chunk_ids = []
            for i, chunk in enumerate(chunks):
                # ä¸ºæ¯ä¸ª chunk ç”Ÿæˆå”¯ä¸€çš„ç¡®å®šæ€§ ID: doc_{id}_chunk_{i}
                chunk_id_str = f"{document.id}_chunk_{i}"
                chunk_uuid = str(uuid.uuid5(NAMESPACE_CATWIKI, chunk_id_str))
                chunk_ids.append(chunk_uuid)

                # ç¡®ä¿ metadata ä¸­åŒ…å« id (è™½ç„¶ base_metadata å·²åŒ…å«ï¼Œä½† double check)
                chunk.metadata["id"] = str(document.id)
                chunk.metadata["chunk_index"] = i

            # 4. æ¸…ç†æ—§æ•°æ® (ä½¿ç”¨ delete_by_metadata)
            # å¿…é¡»åˆ é™¤è¯¥æ–‡æ¡£ ID å…³è”çš„æ‰€æœ‰å‘é‡ï¼Œå› ä¸ºåˆ‡ç‰‡æ•°é‡å¯èƒ½å˜åŒ–
            await vector_store.delete_by_metadata(key="id", value=str(document.id))

            # 5. æ·»åŠ æ–°å‘é‡
            if chunks:
                await vector_store.add_documents(documents=chunks, ids=chunk_ids)

            # æ›´æ–°ä¸º completed çŠ¶æ€
            await crud_document.update_vector_status(
                db, document_id=document_id, status=VectorStatus.COMPLETED
            )

            total_elapsed = time.time() - task_start_time
            logger.info(
                f"âœ¨ [Task] æ–‡æ¡£å‘é‡åŒ–å®Œæˆ! | ID: {document.id} | Chunks: {len(chunks)} | æ€»è€—æ—¶: {total_elapsed:.3f}s"
            )

        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£ {document_id} å‘é‡åŒ–å¤±è´¥: {e}", exc_info=True)
            try:
                await crud_document.update_vector_status(
                    db, document_id=document_id, status=VectorStatus.FAILED, error=str(e)
                )
            except Exception as update_err:
                logger.warning(f"æ›´æ–°æ–‡æ¡£ {document_id} å‘é‡åŒ–çŠ¶æ€å¤±è´¥: {update_err}")


@router.get(
    "", response_model=ApiResponse[PaginatedResponse[Document]], operation_id="listAdminDocuments"
)
async def list_documents(
    page: int = 1,
    size: int = 10,
    site_id: int | None = Query(None, description="ç«™ç‚¹ID"),
    collection_id: int | None = Query(None, description="åˆé›†ID"),
    status: str | None = Query(None, description="çŠ¶æ€è¿‡æ»¤: published, draft"),
    vector_status: str | None = Query(
        None, description="å‘é‡åŒ–çŠ¶æ€è¿‡æ»¤: none, pending, processing, completed, failed"
    ),
    keyword: str | None = Query(None, description="æœç´¢å…³é”®è¯"),
    order_by: str | None = Query(None, description="æ’åºå­—æ®µ: views, updated_at"),
    order_dir: str | None = Query("desc", description="æ’åºæ–¹å‘: asc, desc"),
    exclude_content: bool = Query(True, description="æ˜¯å¦æ’é™¤æ–‡æ¡£å†…å®¹ï¼ˆç”¨äºåˆ—è¡¨å±•ç¤ºï¼Œæå‡æ€§èƒ½ï¼‰"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[PaginatedResponse[Document]]:
    """è·å–æ–‡æ¡£åˆ—è¡¨ï¼ˆåˆ†é¡µï¼‰"""

    paginator = Paginator(page=page, size=size, total=0)  # total ç¨åè®¾ç½®

    # ç»Ÿä¸€æŸ¥è¯¢é€»è¾‘
    if collection_id:
        # è·å–åˆé›†åŠå…¶æ‰€æœ‰å­åˆé›†çš„IDåˆ—è¡¨
        collection_ids = await crud_collection.get_descendant_ids(db, collection_id=collection_id)
    else:
        collection_ids = None

    documents = await crud_document.list(
        db,
        site_id=site_id,
        collection_ids=collection_ids,
        status=status,
        vector_status=vector_status,
        keyword=keyword,
        skip=paginator.skip,
        limit=paginator.size,
        order_by=order_by,
        order_dir=order_dir,
    )
    paginator.total = await crud_document.count(
        db,
        site_id=site_id,
        collection_ids=collection_ids,
        status=status,
        vector_status=vector_status,
        keyword=keyword,
    )

    # æ‰¹é‡åŠ è½½æ‰€æœ‰éœ€è¦çš„collectionä¿¡æ¯ï¼ˆä¼˜åŒ–N+1æŸ¥è¯¢ï¼‰
    collection_ids = list(set([doc.collection_id for doc in documents if doc.collection_id]))
    collection_map = await build_collection_map(db, crud_collection, collection_ids)

    # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ åˆé›†ä¿¡æ¯
    documents_with_collection = []
    for doc in documents:
        doc_dict = await enrich_document_dict(
            doc, db, crud_collection, collection_map=collection_map
        )
        # å¤„ç† exclude_content é€»è¾‘
        if exclude_content:
            doc_dict["content"] = None
        documents_with_collection.append(doc_dict)

    return ApiResponse.ok(
        data=PaginatedResponse(
            list=documents_with_collection,
            pagination=paginator.to_pagination_info(),
        ),
        msg="è·å–æˆåŠŸ",
    )


@router.get("/{document_id}", response_model=ApiResponse[Document], operation_id="getAdminDocument")
async def get_document(
    document_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[Document]:
    """è·å–æ–‡æ¡£è¯¦æƒ…ï¼ˆç®¡ç†åå°æŸ¥çœ‹ï¼Œä¸å¢åŠ æµè§ˆé‡ï¼‰"""
    document = await crud_document.get_with_related_site(db, id=document_id)

    if not document:
        raise NotFoundException(detail=f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨")

    # æ„å»ºæ–‡æ¡£å­—å…¸ï¼Œæ·»åŠ å…³è”çš„åˆé›†ä¿¡æ¯
    document_dict = await enrich_document_dict(
        document, db, crud_collection, include_site_name=True
    )

    return ApiResponse.ok(data=document_dict, msg="è·å–æˆåŠŸ")


@router.post(
    "",
    response_model=ApiResponse[Document],
    status_code=status.HTTP_201_CREATED,
    operation_id="createAdminDocument",
)
async def create_document(
    document_in: DocumentCreate,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[Document]:
    """åˆ›å»ºæ–‡æ¡£"""
    # éªŒè¯ç«™ç‚¹å­˜åœ¨
    site = await crud_site.get(db, id=document_in.site_id)
    if not site:
        raise BadRequestException(detail=f"ç«™ç‚¹ {document_in.site_id} ä¸å­˜åœ¨")

    # åˆ›å»ºæ–‡æ¡£ï¼ˆCRUD å±‚ä¼šè‡ªåŠ¨è®¡ç®—é˜…è¯»æ—¶é—´ï¼‰
    document = await crud_document.create(db, obj_in=document_in)

    # æ›´æ–°ç«™ç‚¹æ–‡ç« è®¡æ•°
    await crud_site.increment_article_count(db, site_id=document_in.site_id)

    # æ„å»ºæ–‡æ¡£å­—å…¸ï¼Œæ·»åŠ å…³è”çš„åˆé›†ä¿¡æ¯
    document_dict = await enrich_document_dict(document, db, crud_collection)

    return ApiResponse.ok(data=document_dict, msg="åˆ›å»ºæˆåŠŸ")


@router.post(
    "/import",
    response_model=ApiResponse[Document],
    status_code=status.HTTP_201_CREATED,
    operation_id="importDocument",
)
async def import_document(
    file: UploadFile = File(...),
    site_id: int = Form(...),
    collection_id: int = Form(...),
    processor_type: str = Form("MinerU"),
    ocr_enabled: bool = Form(False),
    extract_images: bool = Form(False),
    extract_tables: bool = Form(False),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[Document]:
    """
    å¯¼å…¥æ–‡æ¡£ (ä¸Šä¼  -> è§£æ -> åˆ›å»º)
    """
    # 1. éªŒè¯è¾“å…¥
    site = await crud_site.get(db, id=site_id)
    if not site:
        raise BadRequestException(detail=f"ç«™ç‚¹ {site_id} ä¸å­˜åœ¨")

    # éªŒè¯ä¸Šä¼ æ–‡ä»¶ç±»å‹ (ç®€å•éªŒè¯åç¼€)
    filename = file.filename or ""
    suffix = Path(filename).suffix.lower()
    if suffix not in [".pdf", ".jpg", ".jpeg", ".png"]:
        raise BadRequestException(detail="ç›®å‰ä»…æ”¯æŒ PDF å’Œå›¾ç‰‡æ–‡ä»¶")

    # 2. ä¿å­˜ä¸Šä¼ æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            shutil.copyfileobj(file.file, tmp)
            tmp_path = Path(tmp.name)
    except Exception as e:
        logger.error(f"ä¿å­˜ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}")
        raise BadRequestException(detail="æ–‡ä»¶ä¸Šä¼ å¤±è´¥")

    try:
        # 3. è·å–è§£æå™¨é…ç½®
        doc_processor_config = await crud_system_config.get_by_key(
            db, config_key="doc_processor_config"
        )
        if not doc_processor_config:
            raise BadRequestException(detail="ç³»ç»Ÿæœªé…ç½®æ–‡æ¡£å¤„ç†æœåŠ¡ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")

        # æ‰¾åˆ°æŒ‡å®šçš„å¤„ç†å™¨é…ç½®
        config_value = doc_processor_config.config_value
        processors = config_value.get("processors", [])

        target_processor_config = None
        for p in processors:
            if p.get("type") == processor_type or p.get("name") == processor_type:
                target_processor_config = p
                break

        if not target_processor_config:
            # å¦‚æœæ‰¾ä¸åˆ°æŒ‡å®šç±»å‹ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„
            if processors:
                target_processor_config = processors[0]
            else:
                raise BadRequestException(detail=f"æœªæ‰¾åˆ°ç±»å‹ä¸º {processor_type} çš„æ–‡æ¡£å¤„ç†å™¨é…ç½®")

        # è¦†ç›–è®¾ç½® (å¦‚æœæ”¯æŒ)
        if "config" not in target_processor_config:
            target_processor_config["config"] = {}

        # æ³¨å…¥ç”¨æˆ·åœ¨ä¸Šä¼ æ—¶æŒ‡å®šçš„ä¸´æ—¶é…ç½®
        target_processor_config["config"]["is_ocr"] = ocr_enabled
        target_processor_config["config"]["extract_images"] = extract_images
        target_processor_config["config"]["extract_tables"] = extract_tables

        # 4. åˆå§‹åŒ–è§£æå™¨å¹¶æ‰§è¡Œè§£æ
        try:
            # è½¬æ¢ä¸º Schema å¯¹è±¡
            processor_config_obj = DocProcessorConfig(**target_processor_config)
            processor = DocProcessorFactory.create(processor_config_obj)

            logger.info(f"ğŸš€ å¼€å§‹è§£ææ–‡æ¡£: {filename} using {processor_type} (OCR={ocr_enabled})")
            start_time = time.time()

            # æ‰§è¡Œè§£æ
            result = await processor.process(tmp_path)

            elapsed = time.time() - start_time
            logger.info(
                f"âœ… æ–‡æ¡£è§£æå®Œæˆ ({elapsed:.2f}s). Markdown length: {len(result.markdown)}"
            )

        except ValueError as e:
            raise BadRequestException(detail=str(e))
        except Exception as e:
            logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {e}", exc_info=True)
            raise BadRequestException(detail=f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")

        # 5. åˆ›å»ºæ–‡æ¡£
        document_in = DocumentCreate(
            title=filename.replace(suffix, ""),  # é»˜è®¤ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡é¢˜
            content=result.markdown,
            site_id=site_id,
            collection_id=collection_id,
            author=current_user.name or current_user.email,  # é»˜è®¤ä¸ºå½“å‰ç”¨æˆ·
            status=DocumentStatus.DRAFT,  # é»˜è®¤ä¸ºè‰ç¨¿ï¼Œè®©ç”¨æˆ·ç¡®è®¤
        )

        document = await crud_document.create(db, obj_in=document_in)

        # æ›´æ–°ç«™ç‚¹æ–‡ç« è®¡æ•°
        await crud_site.increment_article_count(db, site_id=site_id)

        # æ„å»ºè¿”å›
        document_dict = await enrich_document_dict(document, db, crud_collection)
        return ApiResponse.ok(data=document_dict, msg="æ–‡æ¡£å¯¼å…¥æˆåŠŸ")

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {tmp_path}, {e}")


@router.put(
    "/{document_id}", response_model=ApiResponse[Document], operation_id="updateAdminDocument"
)
async def update_document(
    document_id: int,
    document_in: DocumentUpdate,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[Document]:
    """æ›´æ–°æ–‡æ¡£"""
    document = await crud_document.get(db, id=document_id)
    if not document:
        raise NotFoundException(detail=f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨")

    # æ›´æ–°æ–‡æ¡£ï¼ˆCRUD å±‚ä¼šè‡ªåŠ¨è®¡ç®—é˜…è¯»æ—¶é—´ï¼‰
    document = await crud_document.update(db, db_obj=document, obj_in=document_in)

    # æ„å»ºæ–‡æ¡£å­—å…¸ï¼Œæ·»åŠ å…³è”çš„åˆé›†ä¿¡æ¯
    document_dict = await enrich_document_dict(document, db, crud_collection)

    return ApiResponse.ok(data=document_dict, msg="æ›´æ–°æˆåŠŸ")


@router.delete(
    "/{document_id}", response_model=ApiResponse[None], operation_id="deleteAdminDocument"
)
async def delete_document(
    document_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[None]:
    """åˆ é™¤æ–‡æ¡£"""
    document = await crud_document.get(db, id=document_id)
    if not document:
        raise NotFoundException(detail=f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨")

    site_id = document.site_id

    # å°è¯•åˆ é™¤å‘é‡æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    try:
        from app.core.vector_store import VectorStoreManager

        vector_store = await VectorStoreManager.get_instance()
        await vector_store.delete_by_metadata(key="id", value=str(document_id))
    except Exception as e:
        logger.warning(f"åˆ é™¤æ–‡æ¡£å‘é‡å¤±è´¥: {e}")

    await crud_document.delete(db, id=document_id)

    # æ›´æ–°ç«™ç‚¹æ–‡ç« è®¡æ•°
    await crud_site.decrement_article_count(db, site_id=site_id)

    return ApiResponse.ok(msg="åˆ é™¤æˆåŠŸ")


# ============ å‘é‡åŒ–ç›¸å…³æ¥å£ ============


@router.post(
    ":batchVectorize",
    response_model=ApiResponse[VectorizeResponse],
    operation_id="batchVectorizeAdminDocuments",
)
async def vectorize_documents(
    request: VectorizeRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[VectorizeResponse]:
    """æ‰¹é‡å‘é‡åŒ–æ–‡æ¡£ï¼ˆå°†æ–‡æ¡£çŠ¶æ€è®¾ç½®ä¸º pendingï¼Œå¹¶å¯åŠ¨å‘é‡åŒ–åå°ä»»åŠ¡ï¼‰"""
    # è¾“å…¥éªŒè¯
    if not request.document_ids:
        raise BadRequestException(detail="æ–‡æ¡£IDåˆ—è¡¨ä¸èƒ½ä¸ºç©º")

    # æ‰¹é‡è·å–æ–‡æ¡£ï¼ˆä¼˜åŒ–ï¼šé¿å… N+1 æŸ¥è¯¢ï¼‰
    documents = await crud_document.get_multi(db, ids=request.document_ids)
    document_map = {doc.id: doc for doc in documents}

    success_ids = []
    failed_count = 0

    for doc_id in request.document_ids:
        document = document_map.get(doc_id)
        if document and can_vectorize(document):
            success_ids.append(doc_id)
        else:
            failed_count += 1

    # æ‰¹é‡æ›´æ–°çŠ¶æ€ï¼ˆä¼˜åŒ–ï¼šä¸€æ¬¡æ‰¹é‡æ›´æ–°ä»£æ›¿å¤šæ¬¡å•ç‹¬æ›´æ–°ï¼‰
    if success_ids:
        await crud_document.batch_update_vector_status(
            db, document_ids=success_ids, status=VectorStatus.PENDING
        )

    # ä¸ºæ¯ä¸ªæˆåŠŸçš„æ–‡æ¡£å¯åŠ¨å¼‚æ­¥åå°ä»»åŠ¡
    for doc_id in success_ids:
        background_tasks.add_task(process_vectorization_task, doc_id)

    return ApiResponse.ok(
        data=VectorizeResponse(
            success_count=len(success_ids), failed_count=failed_count, document_ids=success_ids
        ),
        msg=f"å·²å°† {len(success_ids)} ä¸ªæ–‡æ¡£åŠ å…¥å­¦ä¹ é˜Ÿåˆ—",
    )


@router.post(
    "/{document_id}:vectorize",
    response_model=ApiResponse[Document],
    operation_id="vectorizeAdminDocument",
)
async def vectorize_single_document(
    document_id: int,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[Document]:
    """å‘é‡åŒ–å•ä¸ªæ–‡æ¡£ï¼ˆä¼šå¯åŠ¨å‘é‡åŒ–åå°ä»»åŠ¡ï¼‰"""
    document = await crud_document.get(db, id=document_id)
    if not document:
        raise NotFoundException(detail=f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨")

    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å¯ä»¥å‘é‡åŒ–
    if not can_vectorize(document):
        raise BadRequestException(detail=f"æ–‡æ¡£å½“å‰çŠ¶æ€ä¸º {document.vector_status}ï¼Œæ— æ³•é‡æ–°å­¦ä¹ ")

    # æ›´æ–°çŠ¶æ€ä¸º pending
    document = await crud_document.update_vector_status(
        db, document_id=document_id, status=VectorStatus.PENDING
    )

    # å¯åŠ¨å¼‚æ­¥åå°ä»»åŠ¡
    background_tasks.add_task(process_vectorization_task, document_id)

    # æ„å»ºæ–‡æ¡£å­—å…¸
    document_dict = await enrich_document_dict(document, db, crud_collection)

    return ApiResponse.ok(data=document_dict, msg="å·²åŠ å…¥å­¦ä¹ é˜Ÿåˆ—")


@router.post(
    "/{document_id}:removeVector",
    response_model=ApiResponse[Document],
    operation_id="removeAdminDocumentVector",
)
async def remove_document_vector(
    document_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[Document]:
    """ç§»é™¤æ–‡æ¡£å‘é‡ï¼ˆä»å‘é‡åº“åˆ é™¤å¹¶é‡ç½®çŠ¶æ€ä¸º noneï¼‰"""
    document = await crud_document.get(db, id=document_id)
    if not document:
        raise NotFoundException(detail=f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨")

    # å°è¯•ä»å‘é‡åº“åˆ é™¤æ•°æ®
    try:
        from app.core.vector_store import VectorStoreManager

        vector_store = await VectorStoreManager.get_instance()
        # ä½¿ç”¨ delete_by_metadata ç¡®ä¿åˆ é™¤è¯¥æ–‡æ¡£å…³è”çš„æ‰€æœ‰ chunk
        await vector_store.delete_by_metadata(key="id", value=str(document_id))
    except Exception as e:
        logger.warning(f"åˆ é™¤å‘é‡æ•°æ®å¤±è´¥: {e}")

    # æ›´æ–°çŠ¶æ€ä¸º none
    document = await crud_document.update_vector_status(
        db, document_id=document_id, status=VectorStatus.NONE
    )

    document_dict = await enrich_document_dict(document, db, crud_collection)

    return ApiResponse.ok(data=document_dict, msg="å·²ç§»é™¤å‘é‡æ•°æ®")


@router.get(
    "/{document_id}/chunks",
    response_model=ApiResponse[list[dict]],
    operation_id="getAdminDocumentChunks",
)
async def get_document_chunks(
    document_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[list[dict]]:
    """è·å–æ–‡æ¡£çš„å‘é‡åˆ‡ç‰‡ä¿¡æ¯"""
    logger.info(f"ğŸ” [Chunks] Requesting chunks for document_id: {document_id}")
    document = await crud_document.get(db, id=document_id)
    if not document:
        logger.warning(f"âš ï¸ [Chunks] Document {document_id} not found")
        raise NotFoundException(detail=f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨")

    chunks = []
    try:
        from app.core.vector_store import VectorStoreManager

        vector_store = await VectorStoreManager.get_instance()
        chunks = await vector_store.get_chunks_by_metadata(key="id", value=str(document_id))
        logger.info(f"âœ… [Chunks] Found {len(chunks)} chunks for document {document_id}")
    except Exception as e:
        logger.error(f"âŒ [Chunks] Failed to get chunks: {e}", exc_info=True)
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè¿”å›ç©ºåˆ—è¡¨

    return ApiResponse.ok(data=chunks, msg="è·å–æˆåŠŸ")


@router.post(
    "/retrieve", response_model=ApiResponse[VectorRetrieveResult], operation_id="retrieveDocuments"
)
async def retrieve_vectors(
    request: VectorRetrieveRequest,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
) -> ApiResponse[VectorRetrieveResult]:
    """
    è¯­ä¹‰æ£€ç´¢å‘é‡æ•°æ®åº“ (delegates to VectorService)
    """
    try:
        from app.services.vector_service import VectorService

        # è½¬æ¢è¿‡æ»¤å™¨æ ¼å¼ (Schema åº”è¯¥å…¼å®¹ï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œæ˜ç¡®è¿™é‡Œæ˜¯ VectorRetrieveRequest.filter -> VectorRetrieveFilter)
        # å®é™…ä¸Š Pydantic æ¨¡å‹æ˜¯ä¸€è‡´çš„

        results = await VectorService.retrieve(
            query=request.query,
            k=request.k,
            threshold=request.threshold,
            filter=request.filter,
            enable_rerank=request.enable_rerank,
            rerank_k=request.rerank_k,
        )

        return ApiResponse.ok(data=VectorRetrieveResult(list=results), msg="æ£€ç´¢æˆåŠŸ")

    except Exception as e:
        logger.error(f"æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
        return ApiResponse.ok(data=VectorRetrieveResult(list=[]), msg=f"æ£€ç´¢å¤±è´¥: {str(e)}")
