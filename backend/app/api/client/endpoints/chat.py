# Copyright 2024 CatWiki Authors
#
# Licensed under the CatWiki Open Source License (Modified Apache 2.0);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://github.com/CatWiki/CatWiki/blob/main/LICENSE
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""èŠå¤©è¡¥å…¨ç«¯ç‚¹ (LangGraph ReAct ç‰ˆæœ¬)

åŸºäº LangGraph å®ç°çš„ ReAct Agent èŠå¤©æµç¨‹ï¼š
1. Agent è‡ªä¸»å†³å®šæ£€ç´¢ã€æ¨ç†å¾ªç¯
2. ä½¿ç”¨ astream_events å®æ—¶æµå¼è¾“å‡º
3. è‡ªåŠ¨æå–å¼•ç”¨å¹¶åŒæ­¥
"""

import logging
import uuid
import json
import time
from typing import AsyncGenerator, List, Any

from fastapi import APIRouter, Header, HTTPException
from fastapi.responses import StreamingResponse
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage

from app.schemas.chat import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionChoice,
    ChatCompletionUsage,
    ChatMessage,
    ChatCompletionChunk,
    ChatCompletionChunkChoice,
    ChatCompletionChunkDelta,
)
from app.core.dynamic_config import get_dynamic_chat_config
from app.core.graph import create_agent_graph, langchain_to_openai, extract_citations_from_messages
from app.core.checkpointer import get_checkpointer
from app.db.database import AsyncSessionLocal
from app.services.chat_session_service import ChatSessionService

router = APIRouter()
logger = logging.getLogger(__name__)


async def stream_graph_events(
    graph,
    input_state: dict,
    config: dict,
    model_name: str,
    thread_id: str,
) -> AsyncGenerator[str, None]:
    """æµå¼å“åº”ç”Ÿæˆå™¨ - é€‚é… OpenAI SSE æ ¼å¼ï¼ˆå« tool_calls æ”¯æŒï¼‰"""
    full_response = ""
    citations = []
    
    # ç”Ÿæˆå”¯ä¸€çš„ chunk ID å‰ç¼€
    chunk_id_prefix = f"chatcmpl-{uuid.uuid4()}"

    try:
        # ä½¿ç”¨ v1 event æ ¼å¼
        async for event in graph.astream_events(input_state, config, version="v1"):
            kind = event["event"]
            
            # 1. å¤„ç† LLM æµå¼è¾“å‡º (Token)
            if kind == "on_chat_model_stream":
                chunk_data = event["data"]["chunk"]
                chunk_content = chunk_data.content
                
                # å¤„ç†æ–‡æœ¬å†…å®¹
                if chunk_content:
                    full_response += chunk_content
                    
                    # æ„é€  OpenAI å…¼å®¹ chunk
                    chunk = ChatCompletionChunk(
                        id=chunk_id_prefix,
                        object="chat.completion.chunk",
                        created=int(time.time()),
                        model=model_name,
                        choices=[
                            ChatCompletionChunkChoice(
                                index=0,
                                delta=ChatCompletionChunkDelta(content=chunk_content),
                                finish_reason=None,
                            )
                        ],
                    )
                    yield f"data: {chunk.model_dump_json()}\n\n"
                
                # å¤„ç† tool_calls (å¦‚æœå­˜åœ¨)
                # LangChain çš„ AIMessageChunk å¯èƒ½åŒ…å« tool_call_chunks
                if hasattr(chunk_data, "tool_call_chunks") and chunk_data.tool_call_chunks:
                    for tc_chunk in chunk_data.tool_call_chunks:
                        tool_call_delta = {
                            "index": tc_chunk.get("index", 0),
                            "id": tc_chunk.get("id"),
                            "type": "function" if tc_chunk.get("id") else None,
                            "function": {
                                "name": tc_chunk.get("name"),
                                "arguments": tc_chunk.get("args", "")
                            }
                        }
                        # æ¸…ç† None å€¼
                        tool_call_delta = {k: v for k, v in tool_call_delta.items() if v is not None}
                        if tool_call_delta.get("function"):
                            tool_call_delta["function"] = {k: v for k, v in tool_call_delta["function"].items() if v is not None}
                        
                        chunk = ChatCompletionChunk(
                            id=chunk_id_prefix,
                            object="chat.completion.chunk",
                            created=int(time.time()),
                            model=model_name,
                            choices=[
                                ChatCompletionChunkChoice(
                                    index=0,
                                    delta=ChatCompletionChunkDelta(
                                        tool_calls=[tool_call_delta]
                                    ),
                                    finish_reason=None,
                                )
                            ],
                        )
                        yield f"data: {chunk.model_dump_json()}\n\n"
            
            # 2. å·¥å…·å¼€å§‹è°ƒç”¨ - å‘é€çŠ¶æ€æŒ‡ç¤º
            elif kind == "on_tool_start":
                tool_name = event.get("name", "unknown")
                logger.debug(f"ğŸ”§ [Stream] Tool started: {tool_name}")
                # å¯é€‰ï¼šå‘é€è‡ªå®šä¹‰çŠ¶æ€ chunk ä¾›å‰ç«¯æ˜¾ç¤º "æ­£åœ¨æœç´¢..."
                # è¿™ä¸æ˜¯ OpenAI æ ‡å‡†ï¼Œä½†å¯ä½œä¸ºæ‰©å±•
                status_chunk = {"status": "tool_calling", "tool": tool_name}
                yield f"data: {json.dumps(status_chunk)}\n\n"
            
            # 3. ç›‘å¬å·¥å…·è°ƒç”¨ç»“æŸ
            elif kind == "on_tool_end":
                pass

        # å¾ªç¯ç»“æŸï¼Œå¤„ç†æ”¶å°¾å·¥ä½œ
        
        # ä» Checkpoint è·å–æœ€ç»ˆçŠ¶æ€ä»¥æå–å¼•ç”¨
        # æ³¨æ„: astream_events ç»“æŸæ—¶ï¼Œgraph çŠ¶æ€å·²æ›´æ–°
        # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–°çš„ state snapshot æˆ–è€…ä» event history åˆ†æ
        # æœ€ç®€å•æ˜¯é‡æ–°è·å– state
        state_snapshot = await graph.aget_state(config)
        if state_snapshot.values:
            final_messages = state_snapshot.values.get("messages", [])
            citations = extract_citations_from_messages(final_messages)

        # å‘é€ Citations (è‡ªå®šä¹‰åè®®ï¼Œå®¢æˆ·ç«¯éœ€æ”¯æŒ)
        if citations:
            citation_chunk = {"citations": citations}
            yield f"data: {json.dumps(citation_chunk)}\n\n"

        # å‘é€ [DONE]
        yield "data: [DONE]\n\n"
        
        # 3. å¼‚æ­¥æ›´æ–°æ•°æ®åº“è®°å½• (Side Effect)
        if full_response:
            async with AsyncSessionLocal() as db:
                await ChatSessionService.update_assistant_response(
                    db=db, thread_id=thread_id, assistant_message=full_response
                )

    except Exception as e:
        logger.error(f"âŒ [Chat] Stream error: {e}", exc_info=True)
        # å‘é€é”™è¯¯ä¿¡æ¯ç»™å‰ç«¯
        error_chunk = ChatCompletionChunk(
            id=f"error-{uuid.uuid4()}",
            model=model_name,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(content=f"\n\n[System Error: {str(e)}]"),
                    finish_reason="stop",
                )
            ],
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"


@router.post(
    "/completions", response_model=ChatCompletionResponse, operation_id="createChatCompletion"
)
async def create_chat_completion(
    request: ChatCompletionRequest,
    origin: str | None = Header(None),
    referer: str | None = Header(None),
) -> ChatCompletionResponse | StreamingResponse:
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨ (OpenAI å…¼å®¹æ¥å£)
    """
    # å¦‚æœæœªæŒ‡å®š site_idï¼Œåˆ™è§†ä¸ºå…¨å±€å¤šç«™ç‚¹æ¨¡å¼ (site_id=0)
    site_id = request.filter.site_id if (request.filter and request.filter.site_id) else 0

    return await _process_chat_request(request, site_id)


@router.post(
    "/site-completions",
    response_model=ChatCompletionResponse,
    operation_id="createSiteChatCompletion",
)
async def create_site_chat_completion(
    request: ChatCompletionRequest,
    authorization: str = Header(..., description="Bearer <api_key>"),
) -> ChatCompletionResponse | StreamingResponse:
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨ (ä¸“ç”¨æ¥å£)
    """
    if not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Invalid authorization header format")

    token = authorization.replace("Bearer ", "")

    async with AsyncSessionLocal() as db:
        from app.crud.site import crud_site
        site = await crud_site.get_by_api_token(db, api_token=token)

    if not site:
        raise HTTPException(status_code=401, detail="Invalid API Key")

    return await _process_chat_request(request, site.id)


async def _process_chat_request(
    request: ChatCompletionRequest, site_id: int
) -> ChatCompletionResponse | StreamingResponse:
    """æ ¸å¿ƒèŠå¤©å¤„ç†é€»è¾‘ (ReAct Agent)"""

    # 1. è·å–åŠ¨æ€é…ç½®
    async with AsyncSessionLocal() as db:
        chat_config = await get_dynamic_chat_config(db)

    current_model = chat_config["model"]
    current_api_key = chat_config["apiKey"]
    current_base_url = chat_config["baseUrl"]

    # 2. åˆå§‹åŒ– ChatOpenAI
    # è¿™é‡Œçš„æ¨¡å‹å‚æ•°éœ€è¦ä¸ conf/config.py æˆ– åŠ¨æ€é…ç½®ä¿æŒä¸€è‡´
    llm = ChatOpenAI(
        model=current_model,
        api_key=current_api_key,
        base_url=current_base_url,
        temperature=request.temperature or 0.7,
        streaming=True, # å¯ç”¨æµå¼
    )

    # 3. è®°å½•æ—¥å¿—
    msg_preview = request.message[:200] + "..." if len(request.message) > 200 else request.message
    log_banner = (
        "\n"
        "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  AI Chat Request (ReAct) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n"
        f"â”‚ ğŸ¤– Model    : {current_model:<50} â”‚\n"
        f"â”‚ ğŸŒŠ Stream   : {str(request.stream):<50} â”‚\n"
        f"â”‚ ğŸ§µ Thread   : {request.thread_id:<50} â”‚\n"
        f"â”‚ ğŸ¢ Site ID  : {site_id:<50} â”‚\n"
        "â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n"
        f"â”‚ ğŸ—¨ï¸  Message: {msg_preview[:60]:<60} â”‚\n"
        "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
    )
    print(log_banner)

    # 4. åˆ›å»º/æ›´æ–°æ•°æ®åº“ä¼šè¯è®°å½•
    async with AsyncSessionLocal() as db:
        await ChatSessionService.create_or_update(
            db=db,
            thread_id=request.thread_id,
            site_id=site_id,
            user_message=request.message,
            member_id=request.user, 
        )

    # 5. å‡†å¤‡ Agent
    # ä½¿ç”¨ checkpointer ç®¡ç†çŠ¶æ€
    checkpointer_cm = get_checkpointer()
    checkpointer = await checkpointer_cm.__aenter__() # æ‰‹åŠ¨ enter ä»¥ä¾¿åç»­ä½¿ç”¨
    
    try:
        graph = create_agent_graph(checkpointer=checkpointer, model=llm)
        
        # æ„é€ åˆå§‹çŠ¶æ€
        initial_state = {
            "messages": [HumanMessage(content=request.message)],
            # å…¶ä»–çŠ¶æ€å­—æ®µæ ¹æ® graph_state.py å¦‚æœæœ‰é»˜è®¤å€¼å¯çœç•¥ï¼Œæˆ–åœ¨æ­¤åˆå§‹åŒ–
            "site_id": site_id,
        }
        
        config = {"configurable": {"thread_id": request.thread_id}}

        # 6. å¤„ç†è¯·æ±‚
        if request.stream:
            # æµå¼å“åº”ï¼šè¿”å› StreamingResponse
            # æ³¨æ„ï¼šStreamingResponse ä¼šåœ¨åå°è¿è¡Œ generatorï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ­¤å¤„ä¸å…³é—­ checkpointer
            # ä½† checkpointer éœ€è¦å…³é—­... è¿™æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚
            # è§£å†³æ–¹æ¡ˆï¼šåœ¨ generator å†…éƒ¨ç®¡ç† checkpointerï¼Ÿ
            # æˆ–è€…ï¼Œç”±äº postgres checkpointer æ˜¯æ— çŠ¶æ€è¿æ¥æ± ï¼Œä¹Ÿè®¸å¯ä»¥ï¼Ÿ
            # æ›´å¥½çš„åšæ³•ï¼šæŠŠ checkpointer çš„ç”Ÿå‘½å‘¨æœŸäº¤ç»™ generator æˆ–è€…ä¸ä½¿ç”¨ context manager (å¦‚æœå®ƒæ”¯æŒ).
            # è¿™é‡Œæˆ‘ä»¬é‡æ„ stream_generator å†…éƒ¨å»å¤„ç† checkpointer çš„è·å–ã€‚
            
            # ä¸ºäº†é¿å…è¿æ¥æ³„éœ²ï¼Œæˆ‘ä»¬å…ˆå…³é—­è¿™é‡Œçš„ checkpointerï¼Œè®© generator è‡ªå·±å»è·å–
            await checkpointer_cm.__aexit__(None, None, None)
            
            async def protected_generator():
                async with get_checkpointer() as cp:
                    g = create_agent_graph(checkpointer=cp, model=llm)
                    async for chunk in stream_graph_events(g, initial_state, config, current_model, request.thread_id):
                        yield chunk

            return StreamingResponse(
                protected_generator(),
                media_type="text/event-stream",
            )
        
        else:
            # éæµå¼å“åº”
            result = await graph.ainvoke(initial_state, config)
            
            # æå–æœ€åå›å¤
            messages = result["messages"]
            last_message = messages[-1] if messages else AIMessage(content="")
            content = last_message.content if isinstance(last_message, BaseMessage) else ""
            
            # æå–å¼•ç”¨
            citations = extract_citations_from_messages(messages)
            
            # æ›´æ–°æ•°æ®åº“
            async with AsyncSessionLocal() as db:
                await ChatSessionService.update_assistant_response(
                    db=db, thread_id=request.thread_id, assistant_message=content
                )
            
            # æ„é€ å“åº”
            return ChatCompletionResponse(
                id=f"chatcmpl-{uuid.uuid4()}",
                object="chat.completion",
                created=int(time.time()),
                model=current_model,
                choices=[
                    ChatCompletionChoice(
                        index=0,
                        message=ChatMessage(role="assistant", content=content),
                        finish_reason="stop",
                    )
                ],
                usage=None, #è¿™é‡Œç•¥è¿‡ token è®¡ç®—
                # æ³¨æ„ï¼šæ ‡å‡† OpenAI å“åº”ä¸åŒ…å« citations å­—æ®µï¼Œ
                # å¦‚æœå®¢æˆ·ç«¯éœ€è¦ï¼Œé€šå¸¸é€šè¿‡ side-channel æˆ– message extra å­—æ®µã€‚
                # ä½† CatWiki å‰ç«¯å¯èƒ½æœŸæœ›åœ¨ response ä¸­? 
                # æ ¹æ®ä¹‹å‰çš„ä»£ç ï¼Œéæµå¼å¹¶æ²¡æœ‰è¿”å› citations... 
                # æŸ¥çœ‹ä¹‹å‰çš„ä»£ç ï¼šcitations ä¼¼ä¹æ²¡æœ‰è¢«è¿”å›åœ¨ standard response body (Pydantic model) ä¸­ã€‚
                # åªæœ‰æµå¼æœ€åå‘é€äº† citation chunkã€‚
                # æˆ‘ä»¬å¯ä»¥æš‚æ—¶ä¿æŒä¸€è‡´ã€‚
            )
            
            # åˆ«å¿˜äº†å…³é—­ checkpointer
            await checkpointer_cm.__aexit__(None, None, None)

    except Exception as e:
        logger.error(f"âŒ [Chat] Execution Error: {e}", exc_info=True)
        # ç¡®ä¿èµ„æºé‡Šæ”¾
        try:
             await checkpointer_cm.__aexit__(None, None, None)
        except:
            pass
        raise e
