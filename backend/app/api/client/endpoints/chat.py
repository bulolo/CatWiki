import logging
import uuid
import time
import json
import asyncio
from typing import AsyncGenerator

from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse
from openai import AsyncOpenAI, APIError

from app.core.config import settings
from app.schemas.chat import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionChoice,
    ChatCompletionUsage,
    ChatMessage,
    ChatCompletionChunk,
    ChatCompletionChunkChoice,
    ChatCompletionChunkDelta
)

router = APIRouter()
logger = logging.getLogger(__name__)

# Global client removed in favor of dynamic instantiation
# client = AsyncOpenAI(...) 

from app.core.dynamic_config import get_dynamic_chat_config
from app.db.database import AsyncSessionLocal

async def stream_generator(client: AsyncOpenAI, model: str, request: ChatCompletionRequest, citations: list = None) -> AsyncGenerator[str, None]:
    """çœŸå®æµå¼å“åº”ç”Ÿæˆå™¨"""
    try:
        stream = await client.chat.completions.create(
            model=model,
            messages=[msg.model_dump(exclude_none=True) for msg in request.messages],
            stream=True,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            presence_penalty=request.presence_penalty,
            frequency_penalty=request.frequency_penalty,
        )

        async for chunk in stream:
            # å…¼å®¹ OpenAI æ ¼å¼ç›´æ¥é€ä¼ 
            yield f"data: {chunk.model_dump_json()}\n\n"

        # åœ¨ç»“æŸå‰å‘é€ Citations
        if citations:
             citation_chunk = {
                 "citations": citations
             }
             yield f"data: {json.dumps(citation_chunk)}\n\n"

        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"âŒ [Chat] Stream error: {e}")
        # å‘é€é”™è¯¯ä¿¡æ¯ç»™å‰ç«¯
        error_chunk = ChatCompletionChunk(
            id=f"error-{uuid.uuid4()}",
            model=model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(content=f"\n\n[Error: {str(e)}]"),
                    finish_reason="stop"
                )
            ]
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"


@router.post("/completions", response_model=ChatCompletionResponse, operation_id="createChatCompletion")
async def create_chat_completion(
    request: ChatCompletionRequest,
) -> ChatCompletionResponse | StreamingResponse:
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨ (OpenAI å…¼å®¹æ¥å£)
    å¯¹æ¥çœŸå® AI æœåŠ¡
    """
    # 1. è·å–åŠ¨æ€é…ç½®
    async with AsyncSessionLocal() as db:
        chat_config = await get_dynamic_chat_config(db)
    
    current_model = chat_config["model"]
    current_api_key = chat_config["apiKey"]
    current_base_url = chat_config["baseUrl"]

    # å®ä¾‹åŒ–å®¢æˆ·ç«¯ (Per-request)
    client = AsyncOpenAI(
        api_key=current_api_key,
        base_url=current_base_url,
    )

    # è®°å½•è¯·æ±‚ä¿¡æ¯
    # è®°å½•è¯¦ç»†è¯·æ±‚ä¿¡æ¯ (ä½¿ç”¨æ˜¾çœ¼çš„æ ¼å¼)
    last_msg = request.messages[-1].content if request.messages else "No messages"
    last_msg_preview = last_msg[:200] + "..." if len(last_msg) > 200 else last_msg
    
    log_banner = (
        "\n"
        "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  AI Chat Request â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n"
        f"â”‚ ğŸ¤– Model    : {current_model:<46} â”‚\n"
        f"â”‚ ğŸŒŠ Stream   : {str(request.stream):<46} â”‚\n"
        f"â”‚ ğŸ” Filter   : {str(request.filter if request.filter else 'None (Global Mode)'):<46} â”‚\n"
        f"â”‚  Messages : {len(request.messages):<46} â”‚\n"
        "â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n"
        "â”‚ ğŸ—¨ï¸  Config      : {current_base_url} ({current_api_key[:6]}...)     \n"
        "â”‚ ğŸ—¨ï¸  Last Message:                                                â”‚\n"
        f"{last_msg_preview}\n"
        "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
    )
    # ä½¿ç”¨ print ç¡®ä¿ç›´æ¥è¾“å‡ºåˆ°æ§åˆ¶å° (æœ‰æ—¶å€™ logger ä¼šæœ‰æ ¼å¼åŒ–)
    print(log_banner)
    # åŒæ—¶è®°å½•åˆ° logger ä¾›æŒä¹…åŒ–
    logger.info(f"AI Chat Request: model={current_model} filter={request.filter}")

    # å¦‚æœæœ‰ filterï¼Œç›®å‰ä»…æ‰“å°æ—¥å¿—ï¼Œåç»­å¯¹æ¥ RAG
    if request.filter:
        logger.info(f" [Chat] RAG Filter: {request.filter}")

    # RAG: å¦‚æœæœ‰æœ€åä¸€æ¡æ¶ˆæ¯ï¼Œå°è¯•æ£€ç´¢ç›¸å…³æ–‡æ¡£
    context_str = ""
    retrieved_docs = []
    relevant_docs = []
    if request.messages:
        try:
            from app.services.vector_service import VectorService
            
            # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            query = request.messages[-1].content
            
            # æ‰§è¡Œæ£€ç´¢
            logger.info(f"ğŸ” [Chat] Detecting RAG opportunity for query: '{query[:50]}...'")
            retrieved_docs = await VectorService.retrieve(
                query=query,
                k=10,  # å¬å›10æ¡
                # åç»­å¯ä»¥åœ¨ request ä¸­å¢åŠ  enable_rerank å‚æ•°æ§åˆ¶
                # enable_rerank=settings.AI_RERANK_ENABLE (å·²ç§»é™¤ï¼Œç”± VectorService è‡ªåŠ¨åˆ¤æ–­)
            )
            
            if retrieved_docs:
                logger.info(f"ğŸ“š [Chat] Found {len(retrieved_docs)} relevant context docs")
                
                # æŒ‰ document_id åˆ†ç»„
                # åŸå› ï¼šretrieved_docs æ˜¯ç‰‡æ®µçº§åˆ« (chunks)ï¼Œå¯èƒ½å¤šä¸ªç‰‡æ®µå±äºåŒä¸€ä¸ªæ–‡æ¡£ã€‚
                # å¦‚æœç›´æ¥å°†ç‰‡æ®µä½œä¸º [1], [2]... å–‚ç»™ LLMï¼Œä¼šå¯¼è‡´ï¼š
                # 1. LLM å¼•ç”¨ [5]
                # 2. å‰ç«¯å±•ç¤ºçš„æ¥æºåˆ—è¡¨ï¼ˆåŸºäºæ–‡æ¡£å»é‡ï¼‰åªæœ‰ [1], [2], [3]
                # 3. ä»è€Œäº§ç”Ÿå¼•ç”¨åºå·ä¸åŒ¹é…çš„é—®é¢˜
                # 
                # è§£å†³æ–¹æ¡ˆï¼š
                # åœ¨æ„å»º Prompt ä¹‹å‰å…ˆæŒ‰ document_id è¿›è¡Œèšåˆï¼Œå°†åŒä¸€æ–‡æ¡£çš„å¤šä¸ªç‰‡æ®µåˆå¹¶ä¸ºä¸€ä¸ª Context Itemã€‚
                # è¿™æ · Prompt ä¸­çš„ Document [1] å°±ä¸¥æ ¼å¯¹åº”å‰ç«¯å±•ç¤ºçš„ Citation [1]ã€‚
                doc_map = {}
                for doc in retrieved_docs:
                    doc_id = doc.document_id
                    if not doc_id:
                        continue
                        
                    if doc_id not in doc_map:
                        doc_map[doc_id] = {
                            "title": doc.document_title,
                            "content": [],
                            "metadata": doc.metadata,
                            "score": doc.score,
                            "document_id": doc_id
                        }
                    # ä¿ç•™ chunk å†…å®¹
                    doc_map[doc_id]["content"].append(doc.content)
                
                # è½¬æ¢å›åˆ—è¡¨ (ä¿æŒåŸå§‹ç›¸å…³æ€§æ’åºï¼Œå³ç¬¬ä¸€æ¬¡å‡ºç°çš„é¡ºåº)
                relevant_docs = []
                seen_ids = set()
                for doc in retrieved_docs:
                    doc_id = doc.document_id
                    if not doc_id or doc_id in seen_ids:
                        continue
                        
                    seen_ids.add(doc_id)
                    info = doc_map[doc_id]
                    # åˆå¹¶å†…å®¹ï¼Œç”¨çœç•¥å·åˆ†éš”
                    full_content = "\n...\n".join(info["content"])
                    
                    relevant_docs.append({
                        "title": info["title"],
                        "content": full_content,
                        "metadata": info["metadata"],
                        "score": info["score"],
                        "document_id": doc_id
                    })

                logger.info(f"ğŸ“š [Chat] Found {len(retrieved_docs)} chunks -> collapsed to {len(relevant_docs)} unique docs")
                
                # æ„å»ºä¸Šä¸‹æ–‡ Prompt
                context_parts = []
                for i, doc in enumerate(relevant_docs):
                    # æ ¼å¼: [1] Title (Score: 0.95): Content...
                    context_parts.append(
                        f"Document [{i+1}] (Title: {doc['title']})\n"
                        f"{doc['content']}\n"
                    )
                
                context_str = "\n".join(context_parts)
                
                # æ³¨å…¥ System Prompt
                system_prompt = (
                    "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“çš„æ™ºèƒ½ AI åŠ©æ‰‹ã€‚\n"
                    "è¯·ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µæ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
                    "å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·ä½ æ ¹æ®çŸ¥è¯†åº“æ— æ³•å›ç­”ï¼Œä½†ä½ å¯ä»¥å°è¯•æä¾›å¸®åŠ©ã€‚\n"
                    "å¦‚æœç›¸å…³ï¼Œè¯·å§‹ç»ˆå¼•ç”¨æ–‡æ¡£æ ‡é¢˜ã€‚\n\n"
                    "ä¸Šä¸‹æ–‡ä¿¡æ¯:\n"
                    f"{context_str}\n"
                )
                
                # å°† System Message æ’å…¥åˆ°æ¶ˆæ¯åˆ—è¡¨æœ€å‰é¢
                # æˆ–è€…å¦‚æœç¬¬ä¸€æ¡å·²ç»æ˜¯ systemï¼Œåˆ™è¿½åŠ åˆ° content æˆ–è€…æ›¿æ¢
                if request.messages[0].role == "system":
                   # è¿½åŠ åˆ°ç°æœ‰ system prompt
                   request.messages[0].content += f"\n\n{system_prompt}"
                else:
                   # æ’å…¥æ–°çš„ system prompt
                   request.messages.insert(0, ChatMessage(role="system", content=system_prompt))
                   
                logger.debug(f"ğŸ“ [Chat] Context injected into system prompt ({len(context_str)} chars)")
            else:
                logger.info("ğŸ¤· [Chat] No relevant context found above threshold")

        except Exception as e:
            logger.error(f"âŒ [Chat] RAG retrieval failed: {e}", exc_info=True)
            # æ£€ç´¢å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­ naked chat
            
    try:
        if request.stream:
            # å‡†å¤‡å¼•ç”¨æ¥æº (Citations)
            citations = []
            if retrieved_docs:
                try:
                    from app.crud import crud_site
                    
                    # è·å–æ¶‰åŠåˆ°çš„ site_id
                    site_ids = list(set(doc.metadata.get("site_id") for doc in retrieved_docs if doc.metadata.get("site_id")))
                    
                    site_map = {}
                    if site_ids:
                        async with AsyncSessionLocal() as db:
                            sites = await crud_site.get_multi(db, ids=site_ids)
                            site_map = {site.id: site for site in sites}
                    
                    # æ„å»º Citation å¯¹è±¡
                    # æ³¨æ„ï¼šrelevant_docs å·²ç»æ˜¯å”¯ä¸€æ–‡æ¡£åˆ—è¡¨ï¼Œä¸”é¡ºåºä¸ Prompt ä¸­çš„ Context ä¸€è‡´
                    for i, doc in enumerate(relevant_docs):
                        doc_id = doc["document_id"]
                        site_id = doc["metadata"].get("site_id")
                        site = site_map.get(site_id)
                        
                        citations.append({
                            "id": str(doc_id),
                            "title": doc["title"],
                            "siteId": site_id,
                            "siteName": site.name if site else "Unknown",
                            "siteDomain": site.domain if site else "",
                            "documentId": doc_id,
                            "score": doc["score"]
                        })
                    
                    logger.info(f"ğŸ“ [Chat] Prepared {len(citations)} citations")
                    
                except Exception as e:
                    logger.error(f"âŒ [Chat] Failed to prepare citations: {e}")

            # Pass modified messages (with enhanced system prompt) AND citations
            return StreamingResponse(
                stream_generator(client, current_model, request, citations=citations),
                media_type="text/event-stream"
            )

        # éæµå¼å“åº”
        response = await client.chat.completions.create(
            model=current_model,
            messages=[msg.model_dump(exclude_none=True) for msg in request.messages],
            stream=False,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            presence_penalty=request.presence_penalty,
            frequency_penalty=request.frequency_penalty,
        )
        
        # è½¬æ¢ä¸ºå†…éƒ¨ Schema (è™½ç„¶ç»“æ„åŸºæœ¬ä¸€è‡´ï¼Œä½†ä¸ºäº†ç±»å‹å®‰å…¨)
        return ChatCompletionResponse(
            id=response.id,
            object=response.object,
            created=response.created,
            model=response.model,
            choices=[
                ChatCompletionChoice(
                    index=c.index,
                    message=ChatMessage(
                        role=c.message.role,
                        content=c.message.content or ""
                    ),
                    finish_reason=c.finish_reason
                ) for c in response.choices
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=response.usage.completion_tokens,
                total_tokens=response.usage.total_tokens
            ) if response.usage else None
        )

    except Exception as e:
        logger.error(f"âŒ [Chat] API Error: {e}", exc_info=True)
        # è¿™é‡Œåº”è¯¥è¿”å›æ ‡å‡† HTTP é”™è¯¯ï¼Œç”±å…¨å±€å¼‚å¸¸å¤„ç†å™¨æ•è·
        raise e
