"""èŠå¤©è¡¥å…¨ç«¯ç‚¹ (LangGraph æŒä¹…åŒ–ç‰ˆæœ¬)

åŸºäºŽ LangGraph å®žçŽ°çš„ RAG èŠå¤©æµç¨‹ï¼š
1. ä½¿ç”¨ PostgreSQL Checkpointer æŒä¹…åŒ–ä¼šè¯åŽ†å²
2. ä½¿ç”¨ LangGraph å›¾è¿›è¡Œæ£€ç´¢å’Œæ¶ˆæ¯é¢„å¤„ç†
3. è°ƒç”¨ OpenAI å…¼å®¹ API ç”Ÿæˆå›žç­”
4. æ”¯æŒæµå¼è¾“å‡º
5. è‡ªåŠ¨åˆ›å»º/æ›´æ–° ChatSession è®°å½•
"""

import logging
import uuid
import json
from typing import AsyncGenerator

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from openai import AsyncOpenAI
from langchain_core.messages import HumanMessage

from app.schemas.chat import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionChoice,
    ChatCompletionUsage,
    ChatMessage,
    ChatCompletionChunk,
    ChatCompletionChunkChoice,
    ChatCompletionChunkDelta
)
from app.core.dynamic_config import get_dynamic_chat_config
from app.core.graph import create_agent_graph, langchain_to_openai
from app.core.checkpointer import get_checkpointer
from app.db.database import AsyncSessionLocal
from app.services.chat_session_service import ChatSessionService

router = APIRouter()
logger = logging.getLogger(__name__)


async def stream_generator(
    client: AsyncOpenAI, 
    model: str, 
    messages: list[dict],
    request: ChatCompletionRequest, 
    citations: list = None
) -> AsyncGenerator[str, None]:
    """æµå¼å“åº”ç”Ÿæˆå™¨"""
    try:
        stream = await client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            presence_penalty=request.presence_penalty,
            frequency_penalty=request.frequency_penalty,
        )

        full_response = ""
        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
            yield f"data: {chunk.model_dump_json()}\n\n"

        # åœ¨ç»“æŸå‰å‘é€ Citations
        if citations:
            citation_chunk = {"citations": citations}
            yield f"data: {json.dumps(citation_chunk)}\n\n"

        # æ›´æ–°ä¼šè¯æœ€åŽä¸€æ¡æ¶ˆæ¯
        if full_response:
            async with AsyncSessionLocal() as db:
                await ChatSessionService.update_assistant_response(
                    db=db,
                    thread_id=request.thread_id,
                    assistant_message=full_response
                )

        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"âŒ [Chat] Stream error: {e}")
        error_chunk = ChatCompletionChunk(
            id=f"error-{uuid.uuid4()}",
            model=model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(content=f"\n\n[Error: {str(e)}]"),
                    finish_reason="stop"
                )
            ]
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"


@router.post("/completions", response_model=ChatCompletionResponse, operation_id="createChatCompletion")
async def create_chat_completion(
    request: ChatCompletionRequest,
) -> ChatCompletionResponse | StreamingResponse:
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨ (OpenAI å…¼å®¹æŽ¥å£)
    
    ä½¿ç”¨ PostgreSQL Checkpointer æŒä¹…åŒ–ä¼šè¯åŽ†å²ï¼Œ
    å‰ç«¯åªéœ€ä¼ å…¥ thread_id å’Œå½“å‰æ¶ˆæ¯ã€‚
    """
    # 1. èŽ·å–åŠ¨æ€é…ç½®
    async with AsyncSessionLocal() as db:
        chat_config = await get_dynamic_chat_config(db)
    
    current_model = chat_config["model"]
    current_api_key = chat_config["apiKey"]
    current_base_url = chat_config["baseUrl"]

    # å®žä¾‹åŒ–å®¢æˆ·ç«¯
    client = AsyncOpenAI(
        api_key=current_api_key,
        base_url=current_base_url,
    )

    # è®°å½•è¯·æ±‚ä¿¡æ¯
    msg_preview = request.message[:200] + "..." if len(request.message) > 200 else request.message
    
    log_banner = (
        "\n"
        "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  AI Chat Request (LangGraph) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n"
        f"â”‚ ðŸ¤– Model    : {current_model:<50} â”‚\n"
        f"â”‚ ðŸŒŠ Stream   : {str(request.stream):<50} â”‚\n"
        f"â”‚ ðŸ§µ Thread   : {request.thread_id:<50} â”‚\n"
        "â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n"
        f"â”‚ ðŸ—¨ï¸  Message: {msg_preview[:60]:<60} â”‚\n"
        "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
    )
    print(log_banner)
    logger.info(f"AI Chat Request: model={current_model} thread_id={request.thread_id}")

    # 1.5 åˆ›å»º/æ›´æ–°ä¼šè¯è®°å½•
    site_id = request.filter.site_id if request.filter else 1  # é»˜è®¤ç«™ç‚¹ID
    async with AsyncSessionLocal() as db:
        await ChatSessionService.create_or_update(
            db=db,
            thread_id=request.thread_id,
            site_id=site_id,
            user_message=request.message,
            member_id=None,  # TODO: ä»Žè®¤è¯ä¸Šä¸‹æ–‡èŽ·å–
        )

    # 2. ä½¿ç”¨ LangGraph æ‰§è¡Œ RAG æµç¨‹ï¼ˆå¸¦æŒä¹…åŒ–ï¼‰
    try:
        async with get_checkpointer() as checkpointer:
            graph = create_agent_graph(checkpointer=checkpointer)
            
            # åªä¼ å…¥å½“å‰æ¶ˆæ¯ï¼ŒåŽ†å²ç”± Checkpointer è‡ªåŠ¨ç®¡ç†
            initial_state = {
                "messages": [HumanMessage(content=request.message)],
                "context": "",
                "citations": [],
                "should_retrieve": True,
                "rewritten_query": ""
            }
            
            # é…ç½® thread_id
            config = {"configurable": {"thread_id": request.thread_id}}
            
            logger.info(f"ðŸš€ [Chat] Invoking LangGraph (thread={request.thread_id})...")
            result = await graph.ainvoke(initial_state, config)
        
        # æå–ç»“æžœ
        processed_messages = langchain_to_openai(result["messages"])
        citations = result.get("citations", [])
        
        logger.info(f"âœ… [Chat] LangGraph completed. Citations: {len(citations)}")
        
    except Exception as e:
        logger.error(f"âŒ [Chat] LangGraph error: {e}", exc_info=True)
        # é™çº§ï¼šä»…ä½¿ç”¨å½“å‰æ¶ˆæ¯
        processed_messages = [{"role": "user", "content": request.message}]
        citations = []

    # 3. è°ƒç”¨ LLM ç”Ÿæˆå›žç­”
    try:
        if request.stream:
            return StreamingResponse(
                stream_generator(client, current_model, processed_messages, request, citations=citations),
                media_type="text/event-stream"
            )

        # éžæµå¼å“åº”
        response = await client.chat.completions.create(
            model=current_model,
            messages=processed_messages,
            stream=False,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            presence_penalty=request.presence_penalty,
            frequency_penalty=request.frequency_penalty,
        )
        
        # æ›´æ–°ä¼šè¯æœ€åŽä¸€æ¡æ¶ˆæ¯
        assistant_message = response.choices[0].message.content or ""
        async with AsyncSessionLocal() as db:
            await ChatSessionService.update_assistant_response(
                db=db,
                thread_id=request.thread_id,
                assistant_message=assistant_message
            )
        
        return ChatCompletionResponse(
            id=response.id,
            object=response.object,
            created=response.created,
            model=response.model,
            choices=[
                ChatCompletionChoice(
                    index=c.index,
                    message=ChatMessage(
                        role=c.message.role,
                        content=c.message.content or ""
                    ),
                    finish_reason=c.finish_reason
                ) for c in response.choices
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=response.usage.completion_tokens,
                total_tokens=response.usage.total_tokens
            ) if response.usage else None
        )

    except Exception as e:
        logger.error(f"âŒ [Chat] API Error: {e}", exc_info=True)
        raise e
