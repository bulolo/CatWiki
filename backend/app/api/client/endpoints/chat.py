"""èŠå¤©è¡¥å…¨ç«¯ç‚¹ (LangGraph ç‰ˆæœ¬)

åŸºäºŽ LangGraph å®žçŽ°çš„ RAG èŠå¤©æµç¨‹ï¼š
1. ä½¿ç”¨ LangGraph å›¾è¿›è¡Œæ£€ç´¢å’Œæ¶ˆæ¯é¢„å¤„ç†
2. è°ƒç”¨ OpenAI å…¼å®¹ API ç”Ÿæˆå›žç­”
3. æ”¯æŒæµå¼è¾“å‡º
"""

import logging
import uuid
import json
from typing import AsyncGenerator

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from openai import AsyncOpenAI

from app.schemas.chat import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionChoice,
    ChatCompletionUsage,
    ChatMessage,
    ChatCompletionChunk,
    ChatCompletionChunkChoice,
    ChatCompletionChunkDelta
)
from app.core.dynamic_config import get_dynamic_chat_config
from app.core.graph import rag_graph, messages_to_langchain, langchain_to_openai
from app.db.database import AsyncSessionLocal

router = APIRouter()
logger = logging.getLogger(__name__)


async def stream_generator(
    client: AsyncOpenAI, 
    model: str, 
    messages: list[dict],
    request: ChatCompletionRequest, 
    citations: list = None
) -> AsyncGenerator[str, None]:
    """æµå¼å“åº”ç”Ÿæˆå™¨
    
    Args:
        client: OpenAI å®¢æˆ·ç«¯
        model: æ¨¡åž‹åç§°
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆå·²åŒ…å« RAG ä¸Šä¸‹æ–‡ï¼‰
        request: åŽŸå§‹è¯·æ±‚ï¼ˆç”¨äºŽå‚æ•°ï¼‰
        citations: å¼•ç”¨æ¥æºåˆ—è¡¨
    """
    try:
        stream = await client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            presence_penalty=request.presence_penalty,
            frequency_penalty=request.frequency_penalty,
        )

        async for chunk in stream:
            yield f"data: {chunk.model_dump_json()}\n\n"

        # åœ¨ç»“æŸå‰å‘é€ Citations
        if citations:
            citation_chunk = {"citations": citations}
            yield f"data: {json.dumps(citation_chunk)}\n\n"

        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"âŒ [Chat] Stream error: {e}")
        error_chunk = ChatCompletionChunk(
            id=f"error-{uuid.uuid4()}",
            model=model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(content=f"\n\n[Error: {str(e)}]"),
                    finish_reason="stop"
                )
            ]
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"


@router.post("/completions", response_model=ChatCompletionResponse, operation_id="createChatCompletion")
async def create_chat_completion(
    request: ChatCompletionRequest,
) -> ChatCompletionResponse | StreamingResponse:
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨ (OpenAI å…¼å®¹æŽ¥å£)
    
    ä½¿ç”¨ LangGraph è¿›è¡Œ RAG æ£€ç´¢å’Œæ¶ˆæ¯é¢„å¤„ç†ï¼Œ
    ç„¶åŽè°ƒç”¨é…ç½®çš„ AI æœåŠ¡ç”Ÿæˆå›žç­”ã€‚
    """
    # 1. èŽ·å–åŠ¨æ€é…ç½®
    async with AsyncSessionLocal() as db:
        chat_config = await get_dynamic_chat_config(db)
    
    current_model = chat_config["model"]
    current_api_key = chat_config["apiKey"]
    current_base_url = chat_config["baseUrl"]

    # å®žä¾‹åŒ–å®¢æˆ·ç«¯ (Per-request)
    client = AsyncOpenAI(
        api_key=current_api_key,
        base_url=current_base_url,
    )

    # è®°å½•è¯·æ±‚ä¿¡æ¯
    last_msg = request.messages[-1].content if request.messages else "No messages"
    last_msg_preview = last_msg[:200] + "..." if len(last_msg) > 200 else last_msg
    
    log_banner = (
        "\n"
        "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  AI Chat Request (LangGraph) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n"
        f"â”‚ ðŸ¤– Model    : {current_model:<50} â”‚\n"
        f"â”‚ ðŸŒŠ Stream   : {str(request.stream):<50} â”‚\n"
        f"â”‚ ðŸ” Filter   : {str(request.filter if request.filter else 'None (Global Mode)'):<50} â”‚\n"
        f"â”‚ ðŸ“¨ Messages : {len(request.messages):<50} â”‚\n"
        "â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n"
        f"â”‚ ðŸ—¨ï¸  Last Message: {last_msg_preview[:60]:<60} â”‚\n"
        "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
    )
    print(log_banner)
    logger.info(f"AI Chat Request (LangGraph): model={current_model} filter={request.filter}")

    # 2. ä½¿ç”¨ LangGraph æ‰§è¡Œ RAG æµç¨‹
    try:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        langchain_messages = messages_to_langchain(
            [msg.model_dump(exclude_none=True) for msg in request.messages]
        )
        
        # æž„å»ºåˆå§‹çŠ¶æ€
        initial_state = {
            "messages": langchain_messages,
            "context": "",
            "citations": [],
            "should_retrieve": True,
            "rewritten_query": ""
        }
        
        # æ‰§è¡Œå›¾
        logger.info("ï¿½ [Chat] Invoking LangGraph RAG pipeline...")
        result = await rag_graph.ainvoke(initial_state)
        
        # æå–ç»“æžœ
        processed_messages = langchain_to_openai(result["messages"])
        citations = result.get("citations", [])
        
        logger.info(f"âœ… [Chat] LangGraph completed. Citations: {len(citations)}")
        
    except Exception as e:
        logger.error(f"âŒ [Chat] LangGraph error: {e}", exc_info=True)
        # é™çº§ï¼šä½¿ç”¨åŽŸå§‹æ¶ˆæ¯
        processed_messages = [msg.model_dump(exclude_none=True) for msg in request.messages]
        citations = []

    # 3. è°ƒç”¨ LLM ç”Ÿæˆå›žç­”
    try:
        if request.stream:
            return StreamingResponse(
                stream_generator(client, current_model, processed_messages, request, citations=citations),
                media_type="text/event-stream"
            )

        # éžæµå¼å“åº”
        response = await client.chat.completions.create(
            model=current_model,
            messages=processed_messages,
            stream=False,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            presence_penalty=request.presence_penalty,
            frequency_penalty=request.frequency_penalty,
        )
        
        return ChatCompletionResponse(
            id=response.id,
            object=response.object,
            created=response.created,
            model=response.model,
            choices=[
                ChatCompletionChoice(
                    index=c.index,
                    message=ChatMessage(
                        role=c.message.role,
                        content=c.message.content or ""
                    ),
                    finish_reason=c.finish_reason
                ) for c in response.choices
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=response.usage.completion_tokens,
                total_tokens=response.usage.total_tokens
            ) if response.usage else None
        )

    except Exception as e:
        logger.error(f"âŒ [Chat] API Error: {e}", exc_info=True)
        raise e
