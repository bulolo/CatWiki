# Copyright 2024 CatWiki Authors
#
# Licensed under the CatWiki Open Source License (Modified Apache 2.0);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://github.com/CatWiki/CatWiki/blob/main/LICENSE
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
import time
from typing import Optional, List, Dict, Any
from pydantic import BaseModel

from app.core.config import settings
from app.core.vector_store import VectorStoreManager
from app.core.reranker import reranker
from app.schemas.document import VectorRetrieveResponse, VectorRetrieveFilter

logger = logging.getLogger(__name__)


class VectorService:
    """å‘é‡æ£€ç´¢æœåŠ¡ (RAGçš„æ ¸å¿ƒé€»è¾‘)"""

    @classmethod
    async def retrieve(
        cls,
        query: str,
        k: int = 5,
        threshold: float = 0.0,
        filter: Optional[VectorRetrieveFilter] = None,
        enable_rerank: Optional[bool] = None,
        rerank_k: Optional[int] = None,
    ) -> List[VectorRetrieveResponse]:
        """
        æ‰§è¡Œè¯­ä¹‰æ£€ç´¢ï¼ˆåŒ…å« å¬å› + é‡æ’åºï¼‰
        """
        logger.info(
            "\n"
            + "=" * 80
            + f"\nğŸš€ [VECTOR RETRIEVAL START]\n"
            + f"   Query: '{query}'\n"
            + f"   Params: k={k}, threshold={threshold}\n"
            + f"   Filter: {filter.model_dump() if filter else 'None'}\n"
            + "=" * 80
        )
        start_time = time.time()

        try:
            vector_store = await VectorStoreManager.get_instance()

            # 1. æ„å»ºåŠ¨æ€è¿‡æ»¤å™¨
            filter_dict = {}
            if filter:
                # åªæœ‰å½“ site_id > 0 æ—¶æ‰è¿‡æ»¤ç«™ç‚¹ï¼›0 è¡¨ç¤ºå…¨å±€æœç´¢
                if filter.site_id is not None and filter.site_id > 0:
                    filter_dict["site_id"] = filter.site_id
                if filter.id is not None:
                    filter_dict["id"] = str(filter.id)
                if filter.source is not None:
                    filter_dict["source"] = filter.source

            # 2. å†³å®šæ£€ç´¢æ•°é‡
            # ç¡®ä¿ Reranker é…ç½®æ˜¯æœ€æ–°çš„
            await reranker._ensure_config()

            # å¦‚æœå¯ç”¨äº† Rerankï¼Œåˆå§‹å¬å›æ•°é‡éœ€è¦å¢åŠ 
            recall_k = k
            do_rerank = enable_rerank if enable_rerank is not None else reranker.is_enabled

            if do_rerank:
                recall_k = max(recall_k * 5, 50)  # è‡³å°‘å¬å› 50 æ¡ç”¨äºç²¾æ’
                logger.debug(f"ğŸ” [Retrieve] å¯ç”¨é‡æ’åºï¼Œåˆå§‹å¬å›æ•°é‡: {recall_k}")

            # 3. æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢ (è¿”å›çš„æ˜¯è·ç¦», distance)
            results = await vector_store.similarity_search_with_score(
                query=query, k=recall_k, filter=filter_dict if filter_dict else None
            )

            duration = time.time() - start_time
            logger.debug(
                f"âœ… [Retrieve] å‘é‡å¬å›å®Œæˆ | æ•°é‡: {len(results)} | è€—æ—¶: {duration:.3f}s"
            )

            # 4. åˆæ­¥è¿‡æ»¤ç›¸ä¼¼åº¦é˜ˆå€¼å¹¶è½¬æ¢æ ¼å¼
            candidate_list = []
            for doc, distance in results:
                similarity = 1.0 - distance
                if similarity < threshold:
                    continue

                doc_id_val = doc.metadata.get("id")
                doc_title = doc.metadata.get("title")

                candidate_list.append(
                    {
                        "content": doc.page_content,
                        "score": similarity,
                        "document_id": int(doc_id_val) if doc_id_val else 0,
                        "document_title": doc_title,
                        "metadata": doc.metadata,
                        # ä¿ç•™åŸå§‹åˆ†æ•°ä»¥ä¾¿è·Ÿè¸ª
                        "original_score": similarity,
                    }
                )

            # 5. æ‰§è¡Œé‡æ’åº (å¦‚æœå¯ç”¨)
            final_list = []
            if do_rerank:
                if candidate_list:
                    final_k = rerank_k or k
                    final_list = await reranker.rerank(
                        query=query, documents=candidate_list, top_n=final_k
                    )
                else:
                    logger.warning("âš ï¸ [Retrieve] å¬å›ç»“æœä¸ºç©ºæˆ–å‡æœªé€šè¿‡é˜ˆå€¼ï¼Œè·³è¿‡é‡æ’åº")
                    final_list = []
            else:
                # å¦‚æœæ²¡å¯ç”¨ Rerankï¼Œç›´æ¥æˆªå– top k
                final_list = candidate_list[:k]

            # 6. è½¬æ¢ä¸ºå“åº”å¯¹è±¡
            response_objects = [VectorRetrieveResponse(**item) for item in final_list]

            # æ—¥å¿—
            log_lines = [f"âœ… [Retrieve] æœ€ç»ˆè¿”å›ç»“æœæ•°: {len(response_objects)}"]
            for i, res in enumerate(response_objects):
                score_str = f"Score={res.score:.4f}"
                if res.original_score is not None and res.score != res.original_score:
                    score_str = f"Original={res.original_score:.4f} -> Final={res.score:.4f}"
                log_lines.append(
                    f"   #{i + 1}: {score_str} | Title: {res.document_title[:40] if res.document_title else 'N/A'}"
                )

            logger.info("\n" + "\n".join(log_lines))

            return response_objects

        except Exception as e:
            logger.error(f"âŒ [Retrieve] æ£€ç´¢æœåŠ¡å¼‚å¸¸: {e}", exc_info=True)
            # æ ¹æ®éœ€æ±‚ï¼Œè¿™é‡Œå¯ä»¥é€‰æ‹©æŠ›å‡ºæˆ–è€…è¿”å›ç©ºåˆ—è¡¨
            # ä¸ºäº†ç¨³å¥æ€§ï¼Œæš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œä½†è®°å½•é”™è¯¯
            return []
